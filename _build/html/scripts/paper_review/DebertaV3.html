
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Deberta V3 &#8212; surdarla-book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css?v=b4b7a797" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/my_css.css?v=20733ca3" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script kind="utterances">

    var commentsRunWhenDOMLoaded = cb => {
    if (document.readyState != 'loading') {
        cb()
    } else if (document.addEventListener) {
        document.addEventListener('DOMContentLoaded', cb)
    } else {
        document.attachEvent('onreadystatechange', function() {
        if (document.readyState == 'complete') cb()
        })
    }
}

var addUtterances = () => {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "https://utteranc.es/client.js";
    script.async = "async";

    script.setAttribute("repo", "surdarla/surdarla.github.io");
    script.setAttribute("issue-term", "pathname");
    script.setAttribute("theme", "github-light");
    script.setAttribute("label", "ğŸ’¬ comment");
    script.setAttribute("crossorigin", "anonymous");

    sections = document.querySelectorAll("div.section");
    if (sections !== null) {
        section = sections[sections.length-1];
        section.appendChild(script);
    }
}
commentsRunWhenDOMLoaded(addUtterances);
</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=36754332"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-BMRNT7D57Q"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BMRNT7D57Q');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BMRNT7D57Q');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'scripts/paper_review/DebertaV3';</script>
    <link rel="canonical" href="https://surdarla.github.io/scripts/paper_review/DebertaV3.html" />
    <link rel="icon" href="../../_static/my_favi.png"/>
    <link rel="author" title="About these documents" href="../../about.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="DeBERTa" href="DebertaV1.html" />
    <link rel="prev" title="nlp" href="nlp.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>
<aside class="bd-header-announcement" aria-label="Announcement">
  <div class="bd-header-announcement__content">Refactoing Ongoing ...</div>
</aside>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../about.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/surdarla-logo_old.png" class="logo__image only-light" alt="surdarla-book - Home"/>
    <script>document.write(`<img src="../../_static/surdarla-logo_old.png" class="logo__image only-dark" alt="surdarla-book - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../about.html">
                    Surdarla ì…ë‹ˆë‹¤
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">CS229 summary</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../CS229/intro.html">intro</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../CS229/1.html">Learning Algorithms</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../CS229/2.html">Linear Regressin &amp; Gradient Descent</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../CS229/2-1.html">practice - torch sklearn numpy</a></li>
</ul>
</details></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Basics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../basics/anal/anal_intro.html">Analysis</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../basics/anal/linearity.html">linearity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../basics/anal/information-value.html">Feature selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../basics/anal/EDA.html">EDA(Exploratory Data Analysis) íƒìƒ‰ì  ë¶„ì„</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../basics/python/python_intro.html">python</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../basics/python/poetry.html">Poetry ì‚¬ìš©í•˜ê¸°</a></li>
<li class="toctree-l2"><a class="reference internal" href="../basics/python/pandas.html">join merge</a></li>

<li class="toctree-l2"><a class="reference internal" href="../basics/python/OOP.html">ê°ì²´ ì§€í–¥ í”„ë¡œê·¸ë˜ë°</a></li>
<li class="toctree-l2"><a class="reference internal" href="../basics/python/rolling.html">rolling</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../basics/dl/DL_intro.html">Deep learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../basics/dl/tokenizing.html">ì»´í“¨í„°ì˜ ì–¸ì–´í‘œí˜„</a></li>
<li class="toctree-l2"><a class="reference internal" href="../basics/dl/NLP_basics.html">Normalization ì •ê·œí™”</a></li>
<li class="toctree-l2"><a class="reference internal" href="../basics/dl/perceptron.html">Perceptron</a></li>
<li class="toctree-l2"><a class="reference internal" href="../basics/dl/attention.html">Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="../basics/dl/softmax.html">softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="../basics/dl/Logistic.html">Logistic</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../basics/js/JS.html">Javascript</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../basics/js/VUE.html">vue</a></li>
<li class="toctree-l2"><a class="reference internal" href="../basics/js/DOM.html">DOM</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../basics/tips/tips_intro.html">Tips</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../basics/tips/markdownlint.html">vscode markdownlint diableing MD~</a></li>
<li class="toctree-l2"><a class="reference internal" href="../basics/tips/snippet_create.html">vscode snippet ë§Œë“¤ê¸°</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../basics/java/java_intro.html">Java</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../basics/java/var_type.html">ë³€ìˆ˜ì™€ ìë£Œí˜•</a></li>
<li class="toctree-l2"><a class="reference internal" href="../basics/java/control_state.html">ì œì–´ë¬¸</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Paper Review</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="vision.html">vision</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="resnet.html">Resnet</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="nlp.html">nlp</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Deberta V3</a></li>
<li class="toctree-l2"><a class="reference internal" href="DebertaV1.html">DeBERTa</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Algorithm</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../algorithm/problem-solving.html">problem-solving</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/1957.html">1957. Delete Characters to Make Fancy String</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/2558.html">2558 : A + B - 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/2530.html">2530 : ì¸ê³µì§€ëŠ¥ ì‹œê³„</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/2117.html">2117 : ì›í˜• ëŒ„ìŠ¤</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/3046.html">3046 : R2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/2164.html">2164 : ì¹´ë“œ2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/2525.html">2525 : ì˜¤ë¸ ì‹œê³„</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/1012.html">1012 : ìœ ê¸°ë† ë°°ì¶”</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/%EC%98%B9%EC%95%8C%EC%9D%B4%281%29.html">ì˜¹ì•Œì´(1)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/%EA%B0%99%EC%9D%80%EC%88%AB%EC%9E%90%EB%8A%94%EC%8B%AB%EC%96%B4.html">ê°™ì€ ìˆ«ìëŠ” ì‹«ì–´</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/%EC%95%88%EC%A0%84%EC%A7%80%EB%8C%80.html">ì•ˆì „ì§€ëŒ€</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/%EA%B3%B5%EB%8D%98%EC%A7%80%EA%B8%B0.html">ê³µë˜ì§€ê¸°</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/%ED%95%98%EB%85%B8%EC%9D%B4%EC%9D%98%20%ED%83%91.html">í•˜ë…¸ì´ì˜ íƒ‘</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/%EC%9D%98%EC%83%81.html">ì˜ìƒ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/10825.html">10825 : êµ­ì˜ìˆ˜</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/23300.html">23300 : ì›¹ ë¸Œë¼ìš°ì € 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/%EC%98%AC%EB%B0%94%EB%A5%B8%EA%B4%84%ED%98%B8.html">ì˜¬ë°”ë¥¸ ê´„í˜¸</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/10816.html">10816 : ìˆ«ì ì¹´ë“œ 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/21921.html">21921 : ë¸”ë¡œê·¸</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/1535.html">1535 : ì•ˆë…•</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/9663.html">9663 : N-Queen</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/1006.html">1006 : ìŠµê²©ì ì´ˆë¼ê¸°</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/24465.html">24465 : ë°ë·”ì˜ ê¿ˆ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/1065.html">1065 : í•œìˆ˜</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/17952.html">17952 : ê³¼ì œëŠ” ëë‚˜ì§€ ì•Šì•„!</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/20207.html">20207 : ë‹¬ë ¥</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/1781.html">1781 : ì»µë¼ë©´</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/12018.html">12018 : Yonsei TOTO</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../algorithm/data_structure.html">data_structure</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/%EC%9E%90%EB%A3%8C%EA%B5%AC%EC%A1%B0.html">ìë£Œêµ¬ì¡°</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/type_in_python.html">Type in python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/Search.html">Search íƒìƒ‰</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/%EC%B5%9C%EC%86%8C%EB%B9%84%EC%9A%A9%EA%B2%BD%EB%A1%9C.html">ìµœì†Œë¹„ìš©ê²½ë¡œ ë¬¸ì œ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/%EC%B5%9C%EB%8C%80%EC%9C%A0%EB%9F%89.html">Network Flow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/DP.html">Dynamic Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/%EC%9C%84%EC%83%81%EC%A0%95%EB%A0%AC.html">ìœ„ìƒì •ë ¬ : Topological Sort</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Certificate</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../certificate/SQLD/SQL_intro.html">SQLD ì¤€ë¹„ê³¼ì •</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../certificate/SQLD/1.html">1. ë°ì´í„° ëª¨ë¸ë§ì˜ ì´í•´</a></li>
<li class="toctree-l2"><a class="reference internal" href="../certificate/SQLD/2.html">2. ë°ì´í„° ëª¨ë¸ê³¼ ì„±ëŠ¥</a></li>
<li class="toctree-l2"><a class="reference internal" href="../certificate/SQLD/3.html">3. SQL ê¸°ë³¸</a></li>
<li class="toctree-l2"><a class="reference internal" href="../certificate/SQLD/4.html">4. SQL í™œìš©</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../certificate/SQLD/nomad/sql_intro.html">SQL intro</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../certificate/SQLD/nomad/sqlite.html">SQLite</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../certificate/Big_data/big_intro.html">ë¹…ë¶„ê¸° ì¤€ë¹„ê³¼ì •</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../certificate/Big_data/1.html">1. ë¹…ë°ì´í„° ë¶„ì„ ê¸°íš</a></li>
<li class="toctree-l2"><a class="reference internal" href="../certificate/Big_data/2.html">2. ë¹…ë°ì´í„° íƒìƒ‰</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/surdarla/surdarla.github.io" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/surdarla/surdarla.github.io/issues/new?title=Issue%20on%20page%20%2Fscripts/paper_review/DebertaV3.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/scripts/paper_review/DebertaV3.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Deberta V3</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">ë¬¸ì œ ì„¤ì •</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-table">Model Table</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#background">Background</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer">1. Transformer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deberta">2. DeBERTa</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#electra">3. ELECTRA</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#masked-language-model-mlm">2.3.1 Masked Language Model(MLM)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#replaced-token-detection-rtd">2.3.2 Replaced Token Detection(RTD)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#debertav3">DeBERTaV3</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deberta-rtd">3.1 DeBERTa + RTD</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#token-embedding-sharing-in-electra">3.2 Token Embedding Sharing (in ELECTRA)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#average-cosine-similiarity-of-word-embeddings-of-the-g-vs-d">??? average cosine similiarity of word embeddings of the G vs D</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-disentangled-embedding-sharing-gdes">3.3 Gradient-Disentangled Embedding Sharing(GDES)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reference">Reference</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="deberta-v3">
<h1>Deberta V3<a class="headerlink" href="#deberta-v3" title="Link to this heading">#</a></h1>
<p>DEBERTAV3: IMPROVING DEBERTA USING ELECTRA-STYLE PRE-TRAINING WITH GRADIENT-DISENTANGLED EMBEDDING SHARING</p>
<p>Published as a conference paper at ICLR 2023 <a class="sd-sphinx-override sd-badge sd-bg-primary sd-bg-text-primary reference external" href="http://arxiv.org/abs/2111.09543"><span>Paper PDF</span></a></p>
<div class="dropdown note admonition">
<p class="admonition-title">Abstract</p>
<p>This paper presents a new pre-trained language model, <code class="docutils literal notranslate"><span class="pre">DeBERTaV3</span></code>, which improves the original DeBERTa model by replacing masked language modeling(MLM) with <code class="docutils literal notranslate"><span class="pre">replaced</span> <span class="pre">token</span> <span class="pre">detection</span> <span class="pre">(RTD)</span></code>, a more sample-efï¬cient pre-training task. Our analysis shows that vanilla embedding sharing in ELECTRA hurts training efï¬ciency and model performance, because the training losses of the discriminator and the generator pull token embeddings in different directions, creating the â€œtug-of-warâ€ dynamics. We thus propose a new gradient-disentangled embedding sharing method that avoids the tug-of-war dynamics, improving both training efï¬ciency and the quality of the pre-trained model. We have pre-trained DeBERTaV3 using the same settings as DeBERTa to demonstrate its exceptional performance on a wide range of downstream natural language understanding (NLU) tasks. Taking the GLUE benchmark with eight tasks as an example, the DeBERTaV3 Large model achieves a 91.37% average score, which is 1.37% higher than DeBERTa and 1.91% higher than ELECTRA, setting a new state-of-the-art (SOTA) among the models with a similar structure. Furthermore, we have pre-trained a multilingual model mDeBERTaV3 and observed a larger improvement over strong baselines compared to English models. For example, the mDeBERTaV3 Base achieves a 79.8% zero-shot cross-lingual accuracy on XNLI and a 3.6% improvement over XLM-R Base, creating a new SOTA on this benchmark. Our models and code are publicly available at <a class="github reference external" href="https://github.com/microsoft/DeBERTa">microsoft/DeBERTa</a>.</p>
</div>
<section id="id1">
<h2>ë¬¸ì œ ì„¤ì •<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<p>PLMs(Pre-trained Language Models)ë¥¼ scaling upì„ í•´ì„œ íŒŒë¼ë¯¸í„°ë¥¼ ìˆ˜ì‹­ì–µ ìˆ˜ë°±ë§Œ ë‹¨ìœ„ë¡œ ëŠ˜ë¦¬ëŠ” ê²ƒì´ í™•ì‹¤í•œ ì„±ëŠ¥í–¥ìƒì´ ìˆì—ˆê³  ì§€ê¸ˆê¹Œì§€ì˜ ì£¼ë„ì ì¸ ë°©ë²•ì´ì—ˆì§€ë§Œ, ë” ì£¼ìš”í•œ ê²ƒì€ parameterë¥¼ ì¤„ì´ê³  computation costë¥¼ ì¤„ì´ëŠ” ê²ƒì´ë¼ê³  ë§í•œë‹¤.</p>
<p>Improving Efficency</p>
<ol class="arabic simple">
<li><p>incorporating disentangled attention(imporoved relative-position encoding mechanism)</p>
<ul class="simple">
<li><p>DeBERTAëŠ” 1.5Bê¹Œì§€ scaling upì„ í•¨ìœ¼ë¡œì¨ SuperGLUEì—ì„œ ì²˜ìŒìœ¼ë¡œ ì‚¬ëŒ performanceë¥¼ ë„˜ì–´ì„°ë‹¤.</p></li>
</ul>
</li>
<li><p><strong>Replaced Token Detection(RTD)</strong> vs Masked language modeling(MLM)</p>
<ul class="simple">
<li><p>proposed by ELECTRA(2020)</p></li>
<li><p>transformer encoderë¥¼ ì˜¤ì—¼ëœ tokenë¥¼ ì˜ˆì¸¡í•˜ëŠ”ë° ì‚¬ìš©í•˜ë˜ BERT(MLM)ì™€ëŠ” ë‹¤ë¥´ê²Œ</p></li>
<li><p>RTDëŠ” generator, discriminator ë¥¼ ì‚¬ìš©í•œë‹¤. generatorëŠ” í—¤ê¹”ë¦¬ëŠ” ì˜¤ì—¼ì„ ë§Œë“¤ì–´ë‚´ê³ , discriminatorëŠ” generatorê°€ ë§Œë“  ì˜¤ì—¼ëœ í† í°ì„ original inputsê³¼ êµ¬ë¶„ì„ í•´ë‚´ë ¤í•œë‹¤. ë§ˆì¹˜ GAN(Generative Adversarial Networks)ë‘ ìƒë‹¹íˆ ë¹„ìŠ·í•œ ë©´ì´ ìˆë‹¤. ì°¨ì´ì ì— ëŒ€í•´ì„œ ë¶„ëª…í•˜ê²Œ í•˜ì.</p></li>
</ul>
</li>
</ol>
<p>ì—¬ê¸°ì„œ ì´ì „ì˜ DeBERTaì—ì„œ V3ë¡œ ë‚˜ì•„ê°€ë©´ì„œ ë°”ë€ ì ìœ¼ë¡œ ë‘ ê°€ì§€ë¡œ ê¼½ëŠ”ë‹¤.
í•˜ë‚˜ëŠ” ìœ„ì—ì„œ ë§í•œ BERTì˜ MLMì„ ELECTRA ìŠ¤íƒ€ì¼ì˜ RTD(where the model is trained as a discriminator to predict whethre a token in the corrupt input is either original or replaced by a generator)ë¡œ ë°”ê¾¸ëŠ” ê²ƒì´ê³ , ë‹¤ë¥¸ í•˜ë‚˜ëŠ” new embedding sharing methodì´ë‹¤. ELECTRAì—ì„œ generator discriminatorëŠ” ê°™ì€ token embeddingì„ ê³µìœ í•œë‹¤.</p>
<p>ê·¸ëŸ¬ë‚˜ ì—°êµ¬ìë“¤ì€ ë³¸ì¸ë“¤ì˜ ì—°êµ¬ì— ë”°ë¥´ë©´ ì´ê²ƒì€ í•™ìŠµ íš¨ìœ¨ì„± ë©´ì´ë‚˜ ëª¨ë¸ì˜ ì„±ëŠ¥ë©´ì—ì„œ ë¶€ì •ì ì¸ ì˜í–¥ì„ ì¤€ë‹¤ê³  ë§í•œë‹¤. <code class="docutils literal notranslate"><span class="pre">training</span> <span class="pre">losses</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">discriminator</span> <span class="pre">and</span> <span class="pre">the</span> <span class="pre">generator</span> <span class="pre">pull</span> <span class="pre">token</span> <span class="pre">embeddings</span> <span class="pre">into</span> <span class="pre">opposite</span> <span class="pre">directions</span></code> ì¦‰ ìƒì„±ê¸°ì™€ ë¶„ë¥˜ê¸°ì˜ í•™ìŠµì˜ ë°©í–¥ì„±ì´ ë°˜ëŒ€ì´ê¸° ë•Œë¬¸ì— í•™ìŠµ lossê°€ ê°ˆíŒ¡ì§ˆíŒ¡í•  ìˆ˜ ë°–ì— ì—†ë‹¤ëŠ” ê²ƒì´ë‹¤. ê·¸ëŸ¼ ìƒê°í•´ë³¼ ìˆ˜ ìˆëŠ” ê²ƒì´ ë’¤ì— ë‘ê°œì˜ lossë¥¼ ë§Œë“¤ì–´ì„œ í•™ìŠµ ë°©í–¥ì„±ì„ ë°˜ëŒ€ë¡œ ê°ˆ ìˆë„ë¡ í–ˆì„ê¹Œ í•˜ëŠ” ì ì„ ìƒê°í•´ ë³¼ ìˆ˜ ìˆê² ë‹¤. token embeddingì„ ë‹¬ë¦¬ ì¤€ë‹¤ëŠ” ê²ƒì´ ì–´ë–¤ ì˜ë¯¸ì¸ì§€ ë’¤ì—ì„œ í™•ì‹¤í•˜ê²Œ ë‚˜ì™€ì•¼ í•  ê²ƒì´ë‹¤. MLMì€ generatorë¥¼ token ì¤‘ì—ì„œ ì„œë¡œ ê´€ë ¨ì´ ìˆì–´ë³´ì´ëŠ” ê°€ê¹Œìš´ ê²ƒë“¤ì„ ì„œë¡œ ì¡ì•„ë‹¹ê¸°ë©´ì„œ í•™-ìŠµì„ ì§„í–‰í•˜ê²Œ ëœë‹¤. í•˜ì§€ë§Œ ë°˜ë©´ì— RTDì˜ discriminatorëŠ” ì˜ë¯¸ì ìœ¼ë¡œ ê°€ê¹Œìš´ tokenì˜ ì‚¬ì´ë¥¼ ìµœëŒ€í•œ ë©€ë¦¬í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì´ì§„ë¶„ë¥˜ ìµœì í™”(ë§ë‹¤ ì•„ë‹ˆë‹¤)ë¥¼ í•˜ê³  pull their embeddingì„ í•˜ê²Œ ë¨ìœ¼ë¡œì¨ ë”ìš± êµ¬ë¶„ì„ ì˜ í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê²ƒì´ë‹¤. ì´ëŸ¬í•œ â€˜ì¤„ë‹¤ë¦¬ê¸° tug-of-warâ€™ì™€ ê°™ì€ ì—­í•™ì´ í•™ìŠµì„ ë§ì¹˜ê³ , ëª¨ë¸ ì„±ëŠ¥ì„ ë–¨ì–´íŠ¸ë¦¬ëŠ” ê²ƒì´ë¼ê³  ë§í•œë‹¤. ê·¸ë ‡ë‹¤ê³  ë¬´ì¡°ê±´ì ìœ¼ë¡œ seperated embeddingì„ í•  ìˆ˜ëŠ” ì—†ëŠ” ê²ƒì´ generatorì˜ embeddingì„ discriminatorì˜ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ taskí•™ìŠµì— í¬í•¨ì‹œí‚¤ëŠ” ê²ƒì´ ë„ì›€ì´ ëœë‹¤ê³  ë§í•˜ëŠ” ë…¼ë¬¸ë„ ìˆì—ˆê¸° ë•Œë¬¸ì´ë‹¤.</p>
<p>ê·¸ë˜ì„œ ê·¸ë“¤ì´ ì œì•ˆí•˜ëŠ” ê²ƒì€ <code class="docutils literal notranslate"><span class="pre">new</span> <span class="pre">gradient-disentangled</span> <span class="pre">embedding</span> <span class="pre">sharing(GDES)</span> <span class="pre">method</span></code>ì´ë‹¤. the generator shares its embeddings with the discriminator but stops the gradients from the discriminator to the generator embeddings. embedding sharingì˜ ì¥ì ë§Œì„ ì·¨í•˜ë˜, ì¤„ë‹¤ë¦¬ê¸° ì—­í•™ì€ í”¼í•  ìˆ˜ ìˆë„ë¡ gradientì˜ íë¦„ì´ discriminatorì—ì„œ generatorë¡œ íë¥´ì§€ëŠ” ì•Šë„ë¡ í•˜ëŠ” ë°©ì‹ì¸ ê²ƒì´ë‹¤.</p>
<section id="model-table">
<h3>Model Table<a class="headerlink" href="#model-table" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Vocabulary(K)</p></th>
<th class="head"><p>Backbone Parameters(M)</p></th>
<th class="head"><p>Hidden Size</p></th>
<th class="head"><p>Layers</p></th>
<th class="head"><p>Note</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong><a class="reference external" href="https://huggingface.co/microsoft/deberta-v2-xxlarge">V2-XXLarge</a><sup>1</sup></strong></p></td>
<td><p>128</p></td>
<td><p>1320</p></td>
<td><p>1536</p></td>
<td><p>48</p></td>
<td><p>128K new SPM vocab</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://huggingface.co/microsoft/deberta-v2-xlarge">V2-XLarge</a></p></td>
<td><p>128</p></td>
<td><p>710</p></td>
<td><p>1536</p></td>
<td><p>24</p></td>
<td><p>128K new SPM vocab</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://huggingface.co/microsoft/deberta-xlarge">XLarge</a></p></td>
<td><p>50</p></td>
<td><p>700</p></td>
<td><p>1024</p></td>
<td><p>48</p></td>
<td><p>Same vocab as RoBERTa</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://huggingface.co/microsoft/deberta-large">Large</a></p></td>
<td><p>50</p></td>
<td><p>350</p></td>
<td><p>1024</p></td>
<td><p>24</p></td>
<td><p>Same vocab as RoBERTa</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://huggingface.co/microsoft/deberta-base">Base</a></p></td>
<td><p>50</p></td>
<td><p>100</p></td>
<td><p>768</p></td>
<td><p>12</p></td>
<td><p>Same vocab as RoBERTa</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://huggingface.co/microsoft/deberta-v3-large">DeBERTa-V3-Large</a><sup>2</sup></p></td>
<td><p>128</p></td>
<td><p>304</p></td>
<td><p>1024</p></td>
<td><p>24</p></td>
<td><p>128K new SPM vocab</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://huggingface.co/microsoft/deberta-v3-base">DeBERTa-V3-Base</a><sup>2</sup></p></td>
<td><p>128</p></td>
<td><p>86</p></td>
<td><p>768</p></td>
<td><p>12</p></td>
<td><p>128K new SPM vocab</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://huggingface.co/microsoft/deberta-v3-small">DeBERTa-V3-Small</a><sup>2</sup></p></td>
<td><p>128</p></td>
<td><p>44</p></td>
<td><p>768</p></td>
<td><p>6</p></td>
<td><p>128K new SPM vocab</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://huggingface.co/microsoft/deberta-v3-xsmall">DeBERTa-V3-XSmall</a><sup>2</sup></p></td>
<td><p>128</p></td>
<td><p>22</p></td>
<td><p>384</p></td>
<td><p>12</p></td>
<td><p>128K new SPM vocab</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://huggingface.co/microsoft/mdeberta-v3-base">mDeBERTa-V3-Base</a><sup>2</sup></p></td>
<td><p>250</p></td>
<td><p>86</p></td>
<td><p>768</p></td>
<td><p>12</p></td>
<td><p>250K new SPM vocab, multi-lingual model with 102 languages</p></td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<div><p>ì°¸ì¡°</p>
<ol class="arabic simple">
<li><p>This is the model(89.9) that surpassed T5 11B(89.3) and human performance(89.8) on SuperGLUE for the first time. 128K new SPM vocab.</p></li>
<li><p>These V3 DeBERTa models are deberta models pre-trained with ELECTRA-style objective plus gradient-disentangled embedding sharing which significantly improves the model efficiency.</p></li>
</ol>
</div></blockquote>
</section>
</section>
<section id="background">
<h2>Background<a class="headerlink" href="#background" title="Link to this heading">#</a></h2>
<section id="transformer">
<h3>1. Transformer<a class="headerlink" href="#transformer" title="Link to this heading">#</a></h3>
<p>Transformer ê¸°ë°˜ ì–¸ì–´ëª¨ë¸ë“¤ì€ <span class="math notranslate nohighlight">\(L\)</span>ê°œì˜ transformer blockì´ ìŒ“ì—¬ì§„ í˜•íƒœë¡œ êµ¬ì„±ëœë‹¤. ê° ë¸”ë½ë“¤ì€ multi-head self-attention layerë“¤ì„ í¬í•¨í•˜ê³  ê·¸ ë’¤ë¡œì€ fully-connected positional feed-forward networkê°€ ë’¤ë”°ë¥¸ë‹¤. ê¸°ì¡´ì˜ self-attention ë©”ì»¤ë‹ˆì¦˜ì€ ë‹¨ì–´ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ encodeí•˜ëŠ”ë°ëŠ” ì í•©í•˜ì§€ ì•Šì•˜ë‹¤. ê·¸ë˜ì„œ ê¸°ì¡´ì˜ ì ‘ê·¼ë²•ë“¤ì€ positional biasë¥¼ ê° input word embeddingì— ë”í•¨ìœ¼ë¡œì¨, contentì™€ positionì— ë”°ë¼ì„œ ê°’ì´ ë‹¬ë¼ì§€ëŠ” vectorë¡œ í‘œí˜„í•˜ë ¤ê³  í•˜ì˜€ë‹¤. ì´ positional biasëŠ” absolute position embedding, relative position embedding ë“±ì´ ìˆì—ˆë‹¤. ìƒëŒ€ì  ìœ„ì¹˜ ì„ë² ë”©ì´ ì¢‹ì€ ê²°ê³¼ë“¤ì„ ìµœê·¼ê¹Œì§€ëŠ” ë³´ì—¬ì£¼ê³  ìˆëŠ” ì¶”ì„¸ë¼ê³  í•œë‹¤.</p>
</section>
<section id="deberta">
<h3>2. DeBERTa<a class="headerlink" href="#deberta" title="Link to this heading">#</a></h3>
<p>BERTë¡œë¶€í„° ë‘ ê°€ì§€ ê°œì„ ì ì„ ë³´ì—¬ì¤€ë‹¤. ìš°ì„  DA(Disentengled Attention : ë¶„ë¦¬ëœ ì–´í…ì…˜), ê·¸ë¦¬ê³  enhanced mask decoderì´ë‹¤. ì´ì „ì˜ single vectorë¡œ í•˜ë‚˜ì˜ input wordì˜ ë‚´ìš©ê³¼ ìœ„ì§€ì •ë³´ë¥¼ í‘œí˜„í•˜ë ¤ëŠ” ê²ƒê³¼ëŠ” ë‹¤ë¥¸ê²Œ, DAëŠ” ë‘ ê°œì˜ seperate vectorë¥¼ ì‚¬ìš©í•œë‹¤. <code class="docutils literal notranslate"><span class="pre">one</span> <span class="pre">for</span> <span class="pre">the</span> <span class="pre">content</span> <span class="pre">and</span> <span class="pre">one</span> <span class="pre">for</span> <span class="pre">the</span> <span class="pre">position</span></code>. ê·¸ëŸ¬ë©´ì„œ DA ë©”ì»¤ë‹ˆì¦˜ì˜ ë‹¨ì–´ë“¤ ì‚¬ì´ì˜ attention weightëŠ” disentangled matricesì— ì˜í•´ì„œ ê³„ì‚°ë˜ê³  ì´ëŠ” ê°ê° ë‚´ìš©ê³¼ ìƒëŒ€ì  ìœ„ì¹˜ ë‘ ê°œê°€ ê°ê°ì˜ í–‰ë ¬ë¡œ ë‹¤ë¥´ê²Œ ê³„ì‚°ëœë‹¤.</p>
<p>ê·¸ë¦¬ê³  MLMì— ëŒ€í•´ì„œëŠ” BERTì™€ ë™ì¼í•˜ê²Œ ì‚¬ìš©ëœë‹¤. DAê°€ ì´ë¯¸ ë‚´ìš©ê³¼ ìƒëŒ€ì  ìœ„ì¹˜ì— ëŒ€í•œ ê³ ë¯¼ì´ ë“¤ì–´ê°€ëŠ” ìˆì§€ë§Œ, ì¤‘ìš”í•œ ê²ƒì€ absolute positionì— ëŒ€í•œ ê³ ë¯¼ì€ ì—†ë‹¤. absolute positionì€ ì˜ˆì¸¡ì—ì„œ ê½¤ë‚˜ ì£¼ìš”í•œ ìš”ì†Œì„ìœ¼ë¡œ ì´ëŸ¬í•œ ì ì„ ë³´ì™„í•˜ê¸° ìœ„í•´ì„œ DeBERTaì—ì„œëŠ” enhanced Mask Decoderë¥¼ MLMì„ ë³´ì™„í•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©í•œë‹¤. ì´ëŠ” MLM decoding layerì—ì„œ context wordì— absolute position informationì´ ì¶”ê°€ë¡œ ë“¤ì–´ê°€ëŠ” ë°©ì‹ì´ë‹¤.</p>
</section>
<section id="electra">
<h3>3. ELECTRA<a class="headerlink" href="#electra" title="Link to this heading">#</a></h3>
<section id="masked-language-model-mlm">
<h4>2.3.1 Masked Language Model(MLM)<a class="headerlink" href="#masked-language-model-mlm" title="Link to this heading">#</a></h4>
<p>Large-scale Transformer-based PLMs ëŠ” ë³´í†µ ë§ì€ ì–‘ì˜ í…ìŠ¤íŠ¸ë¡œ ì‚¬ì „í•™ìŠµë˜ë©´ì„œ self-supervision objective(ìê¸°ì£¼ë„í•™ìŠµì˜ ëª©ì )ì¸ MLMì„ ì‹¤í˜„í•˜ê³  ì´ ë§ì¸ ì¦‰ìŠ¨ ë¬¸ë§¥ì„ ì´í•´í•˜ê²Œ ëœë‹¤ëŠ” ê²ƒì´ë‹¤.</p>
<p><span class="math notranslate nohighlight">\(X = \{x_i\}\)</span>ëŠ” í•˜ë‚˜ì˜ sequenceì´ê³  <span class="math notranslate nohighlight">\(\tilde{X}\)</span>ëŠ” 15%ì˜ í† í°ì´ ì˜¤ì—¼(masking)ëœ ê²ƒì´ë‹¤. ëª©í‘œëŠ” ë‹¤ì‹œ reconstruct <span class="math notranslate nohighlight">\(X\)</span>ì´ë‹¤. ë°©ë²•ì€ language modelì„ predicting the masked tokens <span class="math notranslate nohighlight">\(\tilde{x} \text{ conditioned on } \tilde{X}\)</span> í•˜ë©´ì„œ í›ˆë ¨ì‹œí‚¤ëŠ” ê²ƒì´ê³  parameterized by <span class="math notranslate nohighlight">\(\theta\)</span>ì´ë‹¤.</p>
<div class="math notranslate nohighlight">
\[
\max_{\theta}\log p_{\theta}(X|\tilde{X}) = \max_{\theta}\sum_{i\in C} \log p_{\theta}(\tilde{x}_i|\tilde{X})
\]</div>
<p>C : index set of the masked tokens in the sequence<br />
BERTì—ì„œëŠ” 10%ì˜ masked tokensë¥¼ ë°”ê¾¼ ìƒíƒœë¡œ ìœ ì§€í•˜ê³ , ë‹¤ë¥¸ 10%ì˜ ë¬´ì‘ìœ„ ì„ íƒëœ í† í°ì„ ë°”ê¾¸ì—ˆê³ , ë‚˜ë¨¸ì§€ 80%ëŠ” ì•„ì˜ˆ <span class="math notranslate nohighlight">\([MASK]\)</span> tokenìœ¼ë¡œ ìœ ì§€í–ˆë‹¤.</p>
</section>
<section id="replaced-token-detection-rtd">
<h4>2.3.2 Replaced Token Detection(RTD)<a class="headerlink" href="#replaced-token-detection-rtd" title="Link to this heading">#</a></h4>
<p>BERTëŠ” í•˜ë‚˜ì˜ transformer encoderë¥¼ ì‚¬ìš©í–ˆê³ , MLMìœ¼ë¡œ í›ˆë ¨ë˜ì—ˆë‹¤. ì´ì™€ ë‹¤ë¥´ê²Œ ELECTRAëŠ” ë‘ ê°œì˜ transformer encodersë¥¼ ì‚¬ìš©í•˜ë©´ì„œ GANì²˜ëŸ¼ í›ˆë ¨í–ˆë‹¤. Generator encoderëŠ” MLMìœ¼ë¡œ í›ˆë ¨ë˜ì—ˆê³ , discriminator encoderëŠ” <code class="docutils literal notranslate"><span class="pre">token-level</span> <span class="pre">binary</span> <span class="pre">classifier</span></code>ë¡œ í›ˆë ¨ë˜ì—ˆë‹¤. generatorëŠ” input sequenceì—ì„œ ë§ˆìŠ¤í‚¹ëœ tokenì„ ëŒ€ì²´í•  ambiguousí•œ í† í°ì„ ìƒì„±í–ˆë‹¤. ê·¸ë¦¬ê³  ì´ë ‡ê²Œ ë§Œë“¤ì–´ì§„ sequenceëŠ” dicriminatorë¡œ ë“¤ì–´ê°€ì„œ í•´ë‹¹ í† í°ì´ original í† í°ì´ ë§ëŠ”ì§€ ì•„ë‹ˆë©´ generatorê°€ ë§Œë“  í† í°ì¸ì§€ë¥¼ ì´ì§„ ë¶„ë¥˜í•œë‹¤. ê·¸ë¦¬ê³  ì´ ì´ì§„ë¶„ë¥˜í•˜ëŠ” ê²ƒì´ RTDì´ë‹¤. ì—¬ê¸°ì„œ parameterized ë˜ëŠ” ë¶€ë¶„ì´ <span class="math notranslate nohighlight">\(\theta_{G}\)</span>ëŠ” generatorì˜ íŒŒë¼ë¯¸í„°ì´ê³ , <span class="math notranslate nohighlight">\(\theta_{D}\)</span>ëŠ” discriminatorì˜ íŒŒë¼ë¯¸í„°ì´ë‹¤. Loss function of the generatorëŠ” ì•„ë˜ì™€ ê°™ë‹¤.</p>
<div class="math notranslate nohighlight">
\[
L_{MLM} = \mathbb{E} \Big(-\sum_{i \in C}\log p_{\theta_G} \big(\tilde{x}_{i,G} = x_i|\tilde{X}_G \big) \Big)
\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(p_{\theta_G} \big(\tilde{x}_{i,G} = x_i|\tilde{X}_G \big) \Big)\)</span> : ì´ ë¶€ë¶„ì´ Gê°€ <span class="math notranslate nohighlight">\(\tilde{X}_G\)</span>ë¥¼ <span class="math notranslate nohighlight">\(x_i\)</span>ë¡œ ì¬êµ¬ì„±í•  í™•ë¥ ì´ë‹¤.</p></li>
<li><p>íŠ¹íˆë‚˜ maskingëœ token(<span class="math notranslate nohighlight">\(\tilde{X}_G\)</span>)ì€ ì›ë³¸ì—ì„œ randomly maskingí•œ 15%ì´ë‹¤.</p></li>
</ul>
<div class="dropdown note admonition">
<p class="admonition-title">Logì˜ ì„±ì§ˆ</p>
<ul class="simple">
<li><p>logì•ì— ë¶™ì€ - ëŠ” ì¬êµ¬ì„±í•  í™•ë¥ ì´ ë†’ì•„ì§ˆìˆ˜ë¡ generatorê°€ ì¼ì„ ì˜í•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì—, ìœ„ì˜ ì¬êµ¬ì„±í•  í™•ë¥ ì´ ë†’ì•„ì§ˆìˆ˜ë¡ lossê°’ì€ ë‚®ì•„ì§€ëŠ” êµ¬ì„±ì´ ë˜ì–´ì•¼ ëª¨ë¸ì´ ëª©í‘œí•¨ìˆ˜ì— ì˜ê±°í•œ í•™ìŠµì„ ì§„í–‰í•˜ê¸°ì— ë¶™ì€ ê²ƒì´ë‹¤.</p></li>
<li><p>logëŠ” 0~1ì‚¬ì´ì˜ í™•ë¥ ê°’<span class="math notranslate nohighlight">\(\mathbb{E}\)</span>ì„ 0~<span class="math notranslate nohighlight">\(\infty\)</span> ê°’ìœ¼ë¡œ ë³€í™˜í•œë‹¤. ì´ë¥¼ í†µí•´ì„œ underflow(0 * 0 * â€¦ -&gt; 0ì— ì ì ë” ê°€ê¹Œì›Œì§€ëŠ”)ë¥¼ ë°©ì§€í•˜ë©°, ê³±ì…ˆì„ ë§ì…ˆìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì„±ì§ˆì„ ì´ìš©í•œë‹¤.</p></li>
</ul>
</div>
<p>discriminatorì— ë“¤ì–´ê°€ëŠ” input sequencesëŠ” generatorì˜ output probabilityì— ë”°ë¼ì„œ new tokensì´ masked tokens ìë¦¬ë¥¼ ì±„ìš´ì±„ë¡œ ë“¤ì–´ì˜¨ë‹¤. ê·¸ë˜ì„œ iê°€ Cì•ˆì— ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ xì´ê³ (replaced tokenì´ ì•„ë‹ˆë¼ ì•„ì˜ˆ ì›ë³¸ì´ê¸° ë•Œë¬¸), indexê°€ Cì— ìˆëŠ”ê±°ë§Œ ë¹„êµë¥¼í•œë‹¤.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\tilde{x}_{i,D} =
\begin{cases}
\tilde{x}_i \sim p_{\theta_G} \big(&amp;\tilde{x}_{i,G} = x_i|\tilde{X}_G\big), &amp; i\in C    \\
&amp; x_i, &amp;i\notin C
\end{cases}
\end{split}\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\sim\)</span> : sim ê¸°í˜¸ëŠ” â€˜distributed asâ€™ ë˜ëŠ” â€˜has a distribution ofâ€™ë¡œ í•´ì„ë˜ë©°, â€˜íŠ¹ì • í™•ë¥  ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ëŠ” ì˜ë¯¸â€™, â€˜ë”°ë¥¸ë‹¤, ë¶„í¬í•œë‹¤â€™ì…ë‹ˆë‹¤. ì—¬ê¸°ì„œ sim ê¸°í˜¸ëŠ” ì£¼ì–´ì§„ í™•ë¥  ë¶„í¬ <span class="math notranslate nohighlight">\(p_{\theta_G}\)</span>ì— ë”°ë¼ ë³€ìˆ˜ <span class="math notranslate nohighlight">\(\tilde{x}_i\)</span>ê°€ ë¶„í¬í•œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸.</p></li>
<li><p>ë”°ë¼ì„œ, <span class="math notranslate nohighlight">\(\tilde{x}_i \sim p{\theta_G} (\tilde{x}_{i,G} = x_i|\tilde{X}_G)\)</span>ëŠ” ë³€ìˆ˜ <span class="math notranslate nohighlight">\(\tilde{x}_i\)</span>ê°€ ì¡°ê±´ë¶€ í™•ë¥  ë¶„í¬ <span class="math notranslate nohighlight">\(p{\theta_G} (\tilde{x}_{i,G} = x_i|\tilde{X}_G)\)</span>ë¥¼ ë”°ë¥¸ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ ë¶„í¬ëŠ” <span class="math notranslate nohighlight">\(p_{\theta_G}\)</span> íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§€ë©°, <span class="math notranslate nohighlight">\(\tilde{X}_G\)</span>ê°€ ì£¼ì–´ì¡Œì„ ë•Œ <span class="math notranslate nohighlight">\(\tilde{x}_{i,G} = x_i\)</span>ì¸ ì¡°ê±´ë¶€ ë¶„í¬ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ìœ„ ìˆ˜ì‹ì˜ ì „ì²´ì ì¸ ì˜ë¯¸ëŠ”, ì¸ë±ìŠ¤ iê°€ ì§‘í•© Cì— í¬í•¨ë˜ì–´ ìˆìœ¼ë©´, <span class="math notranslate nohighlight">\(\tilde{x}_i\)</span>ëŠ” ì¡°ê±´ë¶€ ë¶„í¬ <span class="math notranslate nohighlight">\(p_{\theta_G} (\tilde{x}_{i,G} = x_i|\tilde{X}_G)\)</span>ë¥¼ ë”°ë¥´ê³ , ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ <span class="math notranslate nohighlight">\(\tilde{x}_i\)</span>ëŠ” <span class="math notranslate nohighlight">\(x_i\)</span>ì™€ ë™ì¼í•˜ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
L_{RTD} = \mathbb{E}\Big( -\sum_{i}\log p_{\theta_D} \big(\mathbb{I}(\tilde{x_{i,D}} = x_i)|\tilde{X}_D,i   \big)  \Big)
\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbb{I}\)</span> : indicator functionì„ ë§í•œë‹¤. <span class="math notranslate nohighlight">\((\tilde{x_{i,D}} = x_i)\)</span>ë¥¼ ì¶©ì¡±í•˜ë©´ 1ì„ ë°˜í™˜í•˜ê³ , ì•„ë‹ˆë©´(<span class="math notranslate nohighlight">\(\tilde{X}_D,i\)</span>) 0ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§í•œë‹¤. ë§¤ìš° ì—„ê²©í•œ í•¨ìˆ˜ë¡œ ë³¼ ìˆ˜ ìˆìœ¼ë©° í›„ë³´ëŠ” â€˜sigmoidâ€™, â€˜tahnâ€™, â€˜ReLUâ€™, â€˜Leaky ReLUâ€™ë“±ì´ ìˆë‹¤.</p></li>
<li><p>ìœ„ discriminatorì˜ loss functionì˜ inputì€ <span class="math notranslate nohighlight">\(\tilde{X_D}\)</span> ì´ë©° ì´ê²ƒì€ ìœ„ì˜ 3ë²ˆ equationì˜ resultì´ë‹¤. ì¦‰ discriminatorì—ì„œ í•œë²ˆ ê±¸ëŸ¬ì ¸ ë‚˜ì˜¨ ê²ƒì´ loss functionì— ë“¤ì–´ê°€ëŠ” ê²ƒì´ë‹¤.</p></li>
</ul>
<p>ì´ì²´ì ì¸ ELECTRAì˜ loss functionì€ ì•„ë˜ì™€ ê°™ì´ ì •ë¦¬í•  ìˆ˜ ìˆë‹¤.
$<span class="math notranslate nohighlight">\(
L = L_{MLM} + \lambda L_{RTD}
\)</span>$</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\lambda\)</span> : discriminator loss functionì— ëŒ€í•œ weightë¥¼ ë‚˜íƒ€ëƒ„. ëª¨ë¸ í•™ìŠµì—ì„œ í•´ë‹¹ ì†ì‹¤ì˜ ì¤‘ìš”ì„±ì„ ì¡°ì ˆí•˜ëŠ”ë° ì‚¬ìš©ë¨. ì—¬ê¸°ì„œëŠ” 50ì„ìœ¼ë¡œ MLMì— ë¹„í•´ì„œ 50ë°°ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë” ê³±í•´ì¤€ë‹¤ëŠ” ì˜ë¯¸ì„ìœ¼ë¡œ RTDì— ì—„ì²­ ì¤‘ìš”ì„±ì„ ë†’ê²Œ ì³ì£¼ëŠ” ê²ƒì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.</p></li>
</ul>
</section>
</section>
</section>
<section id="debertav3">
<h2>DeBERTaV3<a class="headerlink" href="#debertav3" title="Link to this heading">#</a></h2>
<p>DeBERTa + RTD training loss + new weight-sharing method</p>
<section id="deberta-rtd">
<h3>3.1 DeBERTa + RTD<a class="headerlink" href="#deberta-rtd" title="Link to this heading">#</a></h3>
<p>ELECTRAì—ì„œ ê°€ì ¸ì˜¨ RTD, ê·¸ë¦¬ê³  DeBERTa disentangled attention mechanismì˜ í•©ì€ í”„ë¦¬íŠ¸ë ˆì´ë‹ ê³¼ì •ì—ì„œ ê°„ë‹¨í•˜ê³  íš¨ê³¼ì ì¸ ê²ƒìœ¼ë¡œ íŒë³„ë˜ì—ˆë‹¤. ì´ì „ DeBERTaì—ì„œ ì‚¬ìš©ë˜ì—ˆë˜ MLM objectiveë¥¼ RTD objectiveë¡œ ë°”ê¿ˆìœ¼ë¡œì¨ ë”ìš± disentangled attention mechainsmì„ ë”ìš± ê°•í™”í•˜ëŠ” ê²ƒì´ë‹¤.</p>
<p>training ë°ì´í„°ë¡œëŠ” Wikipedia, bookcorpusì˜ ë°ì´í„°ê°€ ì‚¬ìš©ë˜ì—ˆë‹¤. generatorëŠ” discriminatorì™€ ê°™ì€ widthë¥¼ ê°€ì§€ë˜ depthëŠ” ì ˆë°˜ë§Œ ê°€ì ¸ê°„ë‹¤. batch sizeëŠ” 2048ì´ë©° 125,000 stepì´ í›ˆë ¨ë˜ì—ˆë‹¤. learning_rate = 5e-4, warmup_steps = 10,000, ê·¸ë¦¬ê³  ìœ„ì—ì„œ ë§í–ˆë“¯ì´ RTD loss functionì— ê°€ì¤‘ì¹˜ë¥¼ 50ì„ ì¤Œìœ¼ë¡œì„œ optimization hyperparameterë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤.</p>
<p>ê²€ì¦ ë°ì´í„°ë¡œëŠ” MNLI, SQuAD v2.0ì„ ì‚¬ìš©í•˜ì˜€ê³ , ì´ ë°ì´í„°ë“¤ì— ëŒ€í•œ ì •ë¦¬ë„ í•„ìš”í•  ê²ƒì´ë‹¤. ê²°ê³¼ì ìœ¼ë¡œ DeBERTaë¥¼ ì••ë„í•˜ì§€ë§Œ ë”ìš±ë” improvedë  ìˆëŠ” í¬ì¸íŠ¸ë¥¼ ë§í•˜ëŠ” ì§€ì ì´ ìˆë‹¤. token Embedding Sharing(ES) used for RTD(ê¸°ì¡´ì— ì‚¬ìš©ë˜ì—ˆë˜)ë¥¼ new Gradient-Disentangled Embedding Sharing(GDES) methodë¡œ ë°”ê¿ˆìœ¼ë¡œì¨ ë”ìš± ë°œì „ë  ê°€ëŠ¥ì„±ì´ ìˆë‹¤ê³  ë§í•œë‹¤.</p>
</section>
<section id="token-embedding-sharing-in-electra">
<h3>3.2 Token Embedding Sharing (in ELECTRA)<a class="headerlink" href="#token-embedding-sharing-in-electra" title="Link to this heading">#</a></h3>
<p>ELECTRAì—ì„œëŠ” generatorì™€ discriminatorê°€ token embeddingì„ ê³µìœ í•œë‹¤. ì´ê²ƒì´ <code class="docutils literal notranslate"><span class="pre">Embedding</span> <span class="pre">Sharing(ES)</span></code>ì´ë‹¤. ì´ ë°©ë²•ì€ generatorê°€ discriminatorì— inputìœ¼ë¡œ ë“¤ì–´ê°ˆ ì •ë³´ë¥¼ ì œê³µí•¨ìœ¼ë¡œì¨ í•™ìŠµì— í•„ìš”í•œ parameterë¥¼ ì¤„ì—¬ì£¼ëŠ” ì—­í• ì„ í•˜ê³  í•™ìŠµì„ ìš©ì´í•˜ê²Œ í•´ì¤€ë‹¤. ê·¸ëŸ¬ë‚˜ ì•ì—ì„œ ë§í–ˆë“¯ì´ ë‘ ê¸°ì œì˜ ëª©ì  ë°©í–¥ì„±ì´ ë°˜ëŒ€ì´ê¸° ë•Œë¬¸ì— ì„œë¡œë¥¼ ë°©í•´í•˜ê³ , í•™ìŠµ ìˆ˜ë ´ì„ ì €í•´í•  ê°€ëŠ¥ì„±ì´ í¬ë‹¤.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(E\)</span> : token embeddings</p></li>
<li><p><span class="math notranslate nohighlight">\(g_E\)</span> : gradients = <span class="math notranslate nohighlight">\(\frac{\delta L_{MLM}}{\delta E} + \lambda\frac{\delta L_{RTD}}{\delta E}\)</span></p></li>
</ul>
<p>ìœ„ì˜ equationì€ token embeddings(E)ê°€ ë‘ ê°œì˜ ì¼ì—ì„œì˜ gradientë¥¼ í•œ ë²ˆì— ì¡°ì •í•˜ë©´ì„œ updateëœë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ìœ„ì—ì„œ ë§í–ˆë“¯ì´ ì´ê²ƒì€ ì¤„ë‹¤ë¦¬ê¸° ì´ë‹¤. ì•„ì£¼ ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ update ì†ë„ë¥¼ ì¡°ì ˆí•˜ë©´ì„œ(small learning_rate, gradient clipping) í•™ìŠµì„ ì§„í–‰í•˜ë©´ ê²°ë¡ ì ìœ¼ë¡œëŠ” ìˆ˜ë ´ì„ í•˜ê¸°ëŠ” í•œë‹¤ê³  ë§í•œë‹¤. í•˜ì§€ë§Œ ë‘ ê°œì˜ taskê°€ ì •ë°˜ëŒ€ì˜ ëª©ì ì„ ê°€ì§„ë‹¤ë©´ ì´ê²ƒì€ êµ‰ì¥íˆ ë¹„íš¨ìœ¨ì ì´ë©°, í•´ë‹¹ ìƒí™©(MLM,RTD)ì€ ì •í™•íˆ ê·¸ëŸ° ìƒí™©ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ë‘ ê°œì˜ taskê°€ token embeddingì„ ì—…ë°ì´íŠ¸ í•˜ë©´ì„œ ë°”ë¼ëŠ” ê²ƒì´ í•˜ë‚˜ëŠ” ìœ ì‚¬ì„±ì— ë”°ë¼ì„œ ê°€ê¹ê²Œ í•˜ê³ , ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ìœ ì‚¬ì„±ì— ë”°ë¼ì„œ ë©€ê²Œí•˜ì—¬ ë¶„ë¥˜í•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì´ë‹¤.</p>
<p>ì´ê²ƒì„ ì‹¤ì œë¡œ í™•ì¸í•˜ê¸° ìœ„í•´ì„œ ì—¬ëŸ¬ ë‹¤ì–‘í•œ ELECTRAë¥¼ êµ¬í˜„í•˜ë˜, í•´ë‹¹ ELECTRAë“¤ì€ token embeddingì„ ê³µìœ í•˜ì§€ ì•Šë„ë¡ êµ¬í˜„í–ˆë‹¤ê³ í•œë‹¤. ê·¸ë ‡ê²Œ êµ¬í˜„ì„ í•˜ë©´ No Embedding Sharing(NES)ê°€ ë˜ëŠ” ê²ƒì´ë‹¤. ì–˜ë„¤ëŠ” gradient updateê°€ ê°ê° ëœë‹¤. ìš°ì„ ì€ (1) generatorì˜ parameter(token embedding with <span class="math notranslate nohighlight">\(E_G\)</span>)ê°€ MLM lossë¥¼ back-propí•˜ë©´ì„œ ì—…ë°ì´íŠ¸ë˜ê³ , (2) ì´í›„ì— discriminatorê°€ generator outputì„ inputìœ¼ë¡œ ë°›ëŠ”ë‹¤ (3) ë§ˆì§€ë§‰ìœ¼ë¡œ discriminator parameter(token embeddings with <span class="math notranslate nohighlight">\(E_D\)</span>)ë¥¼ RTD lossë¥¼ back-propí•˜ë©´ì„œ updateí•œë‹¤.</p>
<p>ì´ë“¤ì€ 3ê°€ì§€ë¡œ ES vs NESë¥¼ ë¹„êµí–ˆë‹¤ê³  í•œë‹¤.</p>
<ol class="arabic simple">
<li><p>convergence speed : NESê°€ ë‹¹ì—°íˆ gradient conflictë¥¼ ë°©ì§€í•¨ìœ¼ë¡œ ìŠ¹ë¦¬</p></li>
<li><p>quality of token embeddings : average cosine similiarity scoresë¥¼ ë¹„êµí–ˆë‹¤. <span class="math notranslate nohighlight">\(E_G\)</span>ì—ì„œëŠ” êµ‰ì¥íˆ íš¨ê³¼ê°€ ì¢‹ì•˜ì§€ë§Œ <span class="math notranslate nohighlight">\(E_D\)</span>ëŠ” í•™ìŠµì„ ê±°ì˜ ëª»í•œ ê²ƒìœ¼ë¡œ ë³´ì˜€ë‹¤. íš¨ê³¼ê°€ ì¢‹ë‹¤ëŠ” ê²ƒì€ ì˜ë¯¸ì ìœ¼ë¡œ coherent ì¼ê´€ì„± ìˆê²Œ <span class="math notranslate nohighlight">\(E_G\)</span>ê°€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ê°€ ë§¤ìš° ë†’ì•„ì§€ëŠ” ê²ƒì„ ë§í•œë‹¤.</p></li>
<li><p>performance on downstream NLP tasks : ë˜í•œ NESê°€ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ testì—ì„œë„ ì¢‹ì€ ëª¨ìŠµì„ ë³´ì´ì§€ ëª»í–ˆë‹¤</p></li>
</ol>
<blockquote>
<div><p>ESê°€ generator embeddingìœ¼ë¡œë¶€í„° discriminatorê°€ í•™ìŠµì„ í•  ë•Œ ë„ì›€ì„ ë°›ëŠ”ê²Œ ì¥ì ì´ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.</p>
</div></blockquote>
</section>
<section id="average-cosine-similiarity-of-word-embeddings-of-the-g-vs-d">
<h3>??? average cosine similiarity of word embeddings of the G vs D<a class="headerlink" href="#average-cosine-similiarity-of-word-embeddings-of-the-g-vs-d" title="Link to this heading">#</a></h3>
<p>average cosine similiarityê°€ ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ê²ƒì¸ê°€?
ì–´ë–¤ ì˜ë¯¸ì¸ì§€ ì œëŒ€ë¡œ ì´í•´ë¥¼ ëª»í•œ ê±° ê°™ë‹¤.</p>
</section>
<section id="gradient-disentangled-embedding-sharing-gdes">
<h3>3.3 Gradient-Disentangled Embedding Sharing(GDES)<a class="headerlink" href="#gradient-disentangled-embedding-sharing-gdes" title="Link to this heading">#</a></h3>
<p>ES, NESì˜ ë‹¨ì ì„ ê½¤ ëš«ê¸° ìœ„í•´ í•´ë‹¹ ë…¼ë¬¸ì—ì„œ ì¤‘ìš”í•˜ê²Œ ë§í•˜ëŠ” ì§€ì ì´ë‹¤. ë‘ ê°œì˜ ì¥ ë‹¨ì ì´ ë¶„ëª…í•˜ê²Œ ì¡´ì¬í•˜ë©´ì„œ ë‘ ê°œë¥¼ ëª¨ë‘ ì±™ê¸¸ ë°©ë²•ìœ¼ë¡œ ë‚˜ì˜¨ ê²ƒì´ë‹¤. í•œ ë²ˆ ì •ë¦¬ë¥¼ í•˜ìë©´ ESëŠ” í•™ìŠµì€ ëŠë¦¬ì§€ë§Œ generator output : token embeddingë¥¼ discriminatorê°€ ì°¸ì¡°í•˜ë©´ì„œ í•™ìŠµ parameter reducingì— ë„ì›€ì„ ë°›ëŠ” ë‹¤ëŠ” ê²ƒì´ë‹¤. í•˜ì§€ë§Œ ë‹¨ì ì€ generator discriminator token embeddingsê°€ ë‘˜ ë‹¤ ì¼ê´€ì„±ì´ ì—†ì–´ì§„ë‹¤ëŠ” ê²ƒì´ë‹¤.</p>
<p>ë°˜ë©´ì— NESëŠ” í•™ìŠµì´ êµ‰ì¥íˆ ë¹¨ë¼ì§„ë‹¤. G,Dì˜ ë°©í–¥ì„±ì˜ ì •ë°˜ëŒ€ì˜ ì„±ì§ˆì„ í•´ê²°í•´ ì¤Œìœ¼ë¡œì¨ í•™ìŠµì´ ìš©ì´í•˜ê²Œ ëœë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ ê²°ë¡ ì ìœ¼ë¡œëŠ” í•™ìŠµì„ ì˜¤íˆë ¤ ESë³´ë‹¤ ëª»í•œ ê¼´ì´ ë‚œë‹¤. ê·¸ë˜ë„ ì¥ì ì€ Gì˜ token embeddingì´ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ê°€ ë†’ì€ ì¼ê´€ì„± ìˆëŠ” embeddingì„ ë§Œë“¤ì–´ì£¼ëŠ” ê²½í–¥ì„±ì„ ë§Œë“œëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤.</p>
<p>ì´ ëª¨ë“  ë‹¨ì ì„ ì»¤ë²„í•˜ê³  ë„ëŒ€ì²´ ì–´ë–»ê²Œ ì¥ì ë§Œ ë‚¨ê¸´ë‹¤ëŠ” ê²ƒì¸ê°€? ì¥ì ë§Œ ë‚¨ê¸´ë‹¤ë©´ í•™ìŠµì˜ ì†ë„ë„ ë¹¨ë¼ì§€ë©´ì„œ ë™ì‹œì— G,Dì˜ token embeddingì´ ìœ ì‚¬ì„±ì„ ìœ ì§€í•˜ëŠ” ê²ƒì´ ë  ê²ƒì´ë‹¤. í›„ìë¥¼ ë…¼ë¬¸ì—ì„œëŠ” â€˜learn from same vocabulary and leverage the rich semantic information encoded in the embeddingsâ€™ë¼ê³  ë§í•œë‹¤.</p>
<p>ì´ê²ƒì„ GDESëŠ” ì˜¤ì§ generator embeddingsë¥¼ MLL lossë§Œ ê°€ì§€ê³  ì—…ë°ì´íŠ¸í•¨ìœ¼ë¡œì¨, outputì˜ ì¼ê´€ì„±ê³¼ í†µì¼ì„±ì„ ìœ ì§€í•œë‹¤. ê·¸ê²ƒì„ ìˆ˜ì‹ì ìœ¼ë¡œ ë³´ë©´</p>
<div class="math notranslate nohighlight">
\[
E_D = sg(E_G) + E_{\Delta}
\]</div>
<p>ì›ë˜ëŠ” NESì—ì„œëŠ” <span class="math notranslate nohighlight">\(E_D\)</span>ë¡œ ë°”ë¡œ backpropí•˜ë˜ ê²ƒì„ re-parameterizeí•˜ì—¬ discriminator embeddingë¥¼ ìƒˆë¡œ ì •ì˜í•œë‹¤. <span class="math notranslate nohighlight">\(sg\)</span>dì˜ ì—­í• ì€ <span class="math notranslate nohighlight">\(E_G\)</span>ì—ì„œ ë‚˜ì˜¨ gradientê°€ ê³„ì† í˜ëŸ¬ë“¤ì–´ê°€ëŠ” ê²ƒì„ ë§‰ê³ , residual embeddings <span class="math notranslate nohighlight">\(E_{\Delta}\)</span> ë§Œì„ ì—…ë°ì´íŠ¸ í•˜ê²Œ í•˜ëŠ” ê²ƒì´ë‹¤. ì´ê²ƒì€ residual learningì—ì„œì˜ ì•„ì´ë””ì–´ì™€ êµ‰ì¥íˆ ìœ ì‚¬í•œ ê²ƒ ê°™ì€ë°!!!.</p>
<p>(1) G output -&gt; input for discriminator = <span class="math notranslate nohighlight">\(E_G\)</span><br />
(2) update <span class="math notranslate nohighlight">\(E_G\)</span> <span class="math notranslate nohighlight">\(E_D\)</span> with MLM loss<br />
(3) D run on G output<br />
(4) update <span class="math notranslate nohighlight">\(E_D\)</span> with RTD loss with only <span class="math notranslate nohighlight">\(E_{\Delta}\)</span><br />
(5) after training, <span class="math notranslate nohighlight">\(E_{\Delta}\)</span> + <span class="math notranslate nohighlight">\(E_G\)</span> = <span class="math notranslate nohighlight">\(E_D\)</span></p>
<blockquote>
<div><p>ì…‹ì€ embedding sharingì˜ ì°¨ì´ë§Œ ìˆê³ , computation costì˜ ì°¨ì´ëŠ” ì—†ìœ¼ë‹¤.
computation costì˜ ì°¨ì´ê°€ ì—†ì´ ì•„ì´ë””ì–´ë§Œìœ¼ë¡œ ì„±ëŠ¥ì˜ upì„ í•œ ê²ƒë„ resnetì´ë‘ ë¹„ìŠ·í•˜ë‹¤.</p>
</div></blockquote>
<p>ì½”ì‚¬ì¸ í‰ê·  ìœ ì‚¬ë„ì—ì„œë„ ì°¨ì´ê°€ NESë³´ë‹¤ëŠ” ëœí•œë° ì´ëŠ” â€˜preserves more semantic information in the discriminator embeddings through the partial weight shargingâ€™ì´ë¼ê³  ë§í•œë‹¤. ì—¬ê¸°ì„œ ë³´ì´ëŠ” partial weight sharingì´ <span class="math notranslate nohighlight">\(E_{\Delta}\)</span>ì´ë©° ì´ê²ƒì´ embeddingì˜ ì”ì°¨ë¥¼ í•™ìŠµí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì§„í–‰ë¨ìœ¼ë¡œì¨ í•™ìŠµì„ ìš©ì´í•˜ê²Œ ê°€ì ¸ê°”ë‹¤â€¦ ì •ë„ë¡œ ë³´ì¸ë‹¤.</p>
</section>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>pre-training paradigm for language models based on the combination of DeBERTa and ELECTRA, two state-of-the-art models that use relative position encoding and replaced token detection (RTD) respectively</p></li>
<li><p>interference issue between the generator and the discriminator in the RTD framework which is well known as the â€œtug-of-warâ€ dynamics.</p></li>
<li><p>GDES : the discriminator to leverage the semantic information encoded in the generatorâ€™s embedding layer without interfering with the generatorâ€™s gradients and thus improves the pre-training efï¬ciency</p></li>
<li><p>a new way of sharing information between the generator and the discriminator in the RTD framework, which can be easily applied to other RTD-based language models</p></li>
<li><p>debertav3-large : 1.37% on the GLUE average score</p></li>
<li><p>ëª©ì  : <code class="docutils literal notranslate"><span class="pre">parameter-efï¬cient</span> <span class="pre">pre-trained</span> <span class="pre">language</span> <span class="pre">models</span></code></p></li>
</ul>
</section>
<section id="reference">
<h2>Reference<a class="headerlink" href="#reference" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://kpmgkr.notion.site/DeBERTa-421579f197f840f48cc920f09cbbb70b">kpmg notion - Deberta Review</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/lighthouse/mdeberta-v3-base-kor-further">HF - lighthouse/mdeberta-v3-base-kor-further</a></p></li>
<li><p><a class="reference external" href="https://github.com/microsoft/DeBERTa">github - microsoft/DeBERTa</a></p></li>
</ol>
<script
   type="text/javascript"
   src="https://utteranc.es/client.js"
   async="async"
   repo="surdarla/surdarla.github.io"
   issue-term="pathname"
   theme="github-light"
   label="ğŸ’¬ comment"
   crossorigin="anonymous"
/></section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./scripts\paper_review"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="nlp.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">nlp</p>
      </div>
    </a>
    <a class="right-next"
       href="DebertaV1.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">DeBERTa</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">ë¬¸ì œ ì„¤ì •</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-table">Model Table</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#background">Background</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer">1. Transformer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deberta">2. DeBERTa</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#electra">3. ELECTRA</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#masked-language-model-mlm">2.3.1 Masked Language Model(MLM)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#replaced-token-detection-rtd">2.3.2 Replaced Token Detection(RTD)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#debertav3">DeBERTaV3</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deberta-rtd">3.1 DeBERTa + RTD</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#token-embedding-sharing-in-electra">3.2 Token Embedding Sharing (in ELECTRA)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#average-cosine-similiarity-of-word-embeddings-of-the-g-vs-d">??? average cosine similiarity of word embeddings of the G vs D</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-disentangled-embedding-sharing-gdes">3.3 Gradient-Disentangled Embedding Sharing(GDES)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reference">Reference</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Surdarla
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      Â© Copyright 2024, Surdarla.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  based on jupyter-book, last updated: 2024-11-01 14:12:36
</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>