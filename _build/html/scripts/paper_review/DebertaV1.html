

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>DeBERTa &#8212; My Jupyter Book</title>



  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>

  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />


  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />

  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-BMRNT7D57Q"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BMRNT7D57Q');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'scripts/paper_review/DebertaV1';</script>
    <link rel="canonical" href="https://surdarla.github.io/scripts/paper_review/DebertaV1.html" />
    <link rel="author" title="About these documents" href="../../about.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="intro" href="../algorithm/intro.html" />
    <link rel="prev" title="Deberta V3" href="DebertaV3.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>


  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">



  <a class="skip-link" href="#main-content">Skip to main content</a>

  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>

  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>

  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">

      <div class="bd-sidebar-primary bd-sidebar">



  <div class="sidebar-header-items sidebar-primary__section">




  </div>

    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">


<a class="navbar-brand logo" href="../../about.html">









    <img src="../../_static/surdarla-logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../../_static/surdarla-logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>


</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">

        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../about.html">
                    Surdarla 입니다
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">CS229 summary</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../CS229/intro.html">intro</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../CS229/1.html">Learning Algorithms</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../CS229/2.html">Linear Regressin &amp; Gradient Descent</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../CS229/2-1.html">practice - torch sklearn numpy</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Basics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../basics/math_intro.html">math collection</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../basics/linear_algebra.html">Linear algebra summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="../basics/information-value.html">Feature selection</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../basics/python_intro.html">python collection</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../basics/poetry.html">Poetry 사용하기</a></li>
<li class="toctree-l2"><a class="reference internal" href="../basics/type_in_python.html">Type in python</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../basics/DL_intro.html">Deep learning collection</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../basics/NLP_basics.html">NLP basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../basics/EDA.html">EDA(Exploratory Data Analysis) 탐색적 분석</a></li>
<li class="toctree-l2"><a class="reference internal" href="../basics/activation_function.html">activation function</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Paper Review</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="vision.html">vision</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="resnet.html">Resnet</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="nlp.html">nlp</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="DebertaV3.html">Deberta V3</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">DeBERTa</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Algorithm</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../algorithm/intro.html">intro</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/2558.html">2558 : A + B - 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/3046.html">3046 : R2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/2164.html">2164 : 카드2</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Certificate</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../certificate/SQLD/SQL_intro.html">SQLD 준비과정</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../certificate/SQLD/1.html">SQLD - 1</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../certificate/Big_data/big_intro.html">빅분기 준비과정</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../certificate/Big_data/1.html">big_data 1</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>


  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>

  <div id="rtd-footer-container"></div>


      </div>

      <main id="main-content" class="bd-main">



<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">

              <div class="bd-header-article">
<div class="header-article-items header-article__inner">

    <div class="header-article-items__start">

        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>

    </div>


    <div class="header-article-items__end">

        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/surdarla/surdarla.github.io" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>


<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">



      <li><a href="../../_sources/scripts/paper_review/DebertaV1.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>


<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>




      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>


<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>

  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>


<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>

    </div>

</div>
</div>



<div id="jb-print-docs-body" class="onlyprint">
    <h1>DeBERTa</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">

            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">문제 설정 Introduction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parallelization">1.1 Parallelization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#disentangled-attention">1.2 Disentangled attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#enhanced-mask-decoder">1.3 Enhanced mask decoder</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#virtual-adversarial-training">1.3 virtual adversarial training</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mlm">1.4 MLM</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deberta-architecture">DeBERTa architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">3.1 Disentangled attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#standard-self-attention-operation">3.1.1 standard self-attention operation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#relative-distance">3.1.2 relative distance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#disentangled-self-attention">3.1.3 disentangled self-attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#projection-matrix">projection matrix</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>



<div id="searchbox"></div>
                <article class="bd-article" role="main">

  <section class="tex2jax_ignore mathjax_ignore" id="deberta">
<h1>DeBERTa<a class="headerlink" href="#deberta" title="Permalink to this heading">#</a></h1>
<p>DEBERTA: DECODING-ENHANCED BERT WITH DIS-ENTANGLED ATTENTION</p>
<p>Published as a conference paper at ICLR 2023 <a class="sd-sphinx-override sd-badge sd-bg-primary sd-bg-text-primary reference external" href="https://arxiv.org/pdf/2006.03654"><span>Paper PDF</span></a></p>
<p>Pengcheng He, Microsoft Dynamics 365 AI<br />
Xiaodong Liu, Microsoft Research<br />
Jianfeng Gao, Microsoft Research<br />
Weizhu Chen, Microsoft Dynamics 365 AI<br />
{penhe,xiaodl,jfgao,wzchen}&#64;microsoft.com</p>
<p>microsoft에서 deberta를 만들었다보니 집필진이 전부 microsoft이다.</p>
<div class="dropdown note admonition">
<p class="admonition-title">Abstract</p>
<p>Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is <strong>the disentangled attention mechanism</strong>, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an <strong>enhanced mask decoder</strong> is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a <strong>new virtual adversarial training method</strong> is used for fine-tuning to improve models’ generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understanding (NLU) and natural langauge generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). Notably, we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters. The significant performance boost makes the single DeBERTa model surpass the human performance on the SuperGLUE benchmark (Wang et al., 2019a) for the first time in terms of macro-average score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the SuperGLUE leaderboard as of January 6, 2021, out performing the human baseline by a decent margin (90.3 versus 89.8).</p>
</div>
<section id="introduction">
<h2>문제 설정 Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">#</a></h2>
<p>Transformer는 nlp에서 가장 효과적인 network architecture로 자리잡았다. 순서에 따라 text를 처리하는 RNN들과 다르게, tranformers는 입력 텍스트(input text)의 모든 단어(every words)에 대해서 self-attention을 적용하여, attention weight를 뽑아낸다. 그리고 이 attention weight는 각 단어들이 서로에게 어떠한 영향을 미치는지를 나타내는 수치이다. 이러한 방식(병렬처리)을 transformer가 사용하고 있기 때문에 RNN과 같은 순차처리 모델보다 large-scale training이 가능한 것이다.</p>
<section id="parallelization">
<h3>1.1 Parallelization<a class="headerlink" href="#parallelization" title="Permalink to this heading">#</a></h3>
<p>RNN과 transformer는 둘 다 sequence modeling architecture이다. 하지만 transformer는 시퀀스의 모든 위치에 대해 동시에 연산을 수행해서 병렬화가 더 쉽다. 병렬화라는 말 자체는 하나의 작업을 여러 개의 작은 자겅ㅂ으로 나누어 동시에 처리함으로써 전체적으로는 하나의 작업을 더 빠르고 효율적으로 수행하는 기술을 말한다. 딥러닝 모델의 학습은 계산량, 데이터량이 많을 수록 좋은 방향으로 흘러가고 있기 때문에 이러한 기술은 필수적이다. GPU를 동시에 사용하거나 데이터를 쪼개거나, 연산을 쪼개거나 하는 방식으로 여러 부분에서 쪼개어 동시다발적으로 수행하는 것을 전반적으로 병렬화라고 말할 수 있다.</p>
<p>병렬화를 가능하게 하는 3가지 특징은 다음과 같다.</p>
<ol class="arabic simple">
<li><p>Attention 메커니즘: Transformer는 Self-Attention 메커니즘을 사용하여 입력 시퀀스의 모든 요소를 동시에 처리할 수 있습니다. 이로 인해 서로 독립적인 계산이 가능하며, 병렬 처리에 적합합니다.</p></li>
<li><p>Layer 단위 병렬화: Transformer의 인코더와 디코더는 여러 층으로 구성되어 있으며, 이러한 층들은 독립적으로 병렬로 계산할 수 있습니다.</p></li>
<li><p>마스킹: 학습 시에는 한 번에 하나의 출력만을 생성하도록 마스킹하여 순차적으로 계산할 수도 있습니다. 하지만 추론(inference) 단계에서는 병렬화하여 동시에 여러 개의 출력을 생성할 수 있습니다.</p></li>
</ol>
</section>
<section id="disentangled-attention">
<h3>1.2 Disentangled attention<a class="headerlink" href="#disentangled-attention" title="Permalink to this heading">#</a></h3>
<blockquote>
<div><p>relative position embedding를 사용하자. 근데 vector를 두 개로 나눠서…</p>
</div></blockquote>
<p><strong>기존의 self-attention의 위치정도 encoding 문제</strong><br />
원래(transformers)의 self-attention mechanism은 주어진 input sequence에서 모든 위치의 정보를 사용하여 output을 생성한다. input sequence의 모든 위치 간의 관계를 높은 수준의 상호의존성을 가진 전연결 그래프로 모델링을 하기 때문에, sequence의 길이가 길어질수록 연산량이 증가하고, 장기의존성을 학습하는데 어려움이 커진다.</p>
<p><strong>positional bias</strong><br />
때문에 기존의 word embedding에 positional bias를 추가해줘서 하나의 벡터에서 content와 position의 정보를 포함하도록 하는 방식이 사용되었다.</p>
<ul class="simple">
<li><p>absolute position embedding : 문장 내 위치만 고려</p></li>
<li><p>relative position embedding : 단어간 상대적 위치만 고려</p></li>
</ul>
<p>기존 BERT를 본 논문에서는 이렇게 말한다. <code class="docutils literal notranslate"><span class="pre">each</span> <span class="pre">word</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">input</span> <span class="pre">layer</span> <span class="pre">is</span> <span class="pre">represented</span> <span class="pre">using</span> <span class="pre">**a</span> <span class="pre">vector**</span> <span class="pre">which</span> <span class="pre">is</span> <span class="pre">the</span> <span class="pre">sum</span> <span class="pre">of</span> <span class="pre">its</span> <span class="pre">word</span> <span class="pre">(content)</span> <span class="pre">embedding</span> <span class="pre">and</span> <span class="pre">(absolute)position</span> <span class="pre">embedding</span></code>. 분명하게 absolute position 정보가 중요한 것은 맞지만 뒤에서 따른 방법으로 이용하되, 여기서는 relative position embedding을 이용하여 위치정보를 input으로 사용하고, 실제로 많은 선제연구들에서 이 방법의 중요성을 말해줬다고 한다.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p>…</p></th>
<th class="head text-center"><p>word in input layer</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>transformer</p></td>
<td class="text-center"><p>word itself</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Bert</p></td>
<td class="text-center"><p>vector(word embedding(token) + position embedding)</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>Deberta</p></td>
<td class="text-center"><p>vector(word embedding(token)),vector(absoulte position embedding)</p></td>
</tr>
</tbody>
</table>
<p><strong>여기서는 relative word embedding을 사용</strong><br />
disentangled attention은 이렇게 각각의 embedding으로 word에 대한 표현을 한 뒤, 각각의 attention weight를 <code class="docutils literal notranslate"><span class="pre">disentangled</span> <span class="pre">matrices</span> <span class="pre">=</span> <span class="pre">2개의</span> <span class="pre">vectors</span> <span class="pre">-&gt;</span> <span class="pre">2x2</span> <span class="pre">=</span> <span class="pre">4개의</span> <span class="pre">matrices</span></code>를 이용해서 계산한다. 이는 단어의 관계라는 것이 의미적 관계만 있는 것이 아니라 상대적인 위치에 의해서도 영향을 받는 것에서 나온 것이다. 본문의 예시는 ‘deep’,’learning’이라는 두 단어가 다른 문장에서 있을 때보다 옆에 붙어있을 때 의존성이 굉장히 높아지는 것으로 든다.</p>
</section>
<section id="enhanced-mask-decoder">
<h3>1.3 Enhanced mask decoder<a class="headerlink" href="#enhanced-mask-decoder" title="Permalink to this heading">#</a></h3>
<p>absolute position embedding 더하기</p>
<p>MLM : masked language modeling은 Bert에서 나왔던 것으로 해당 모델 역시 이 방법을 사용해서 기본적인 pre-training을 진행한다. MLM 이라는 것은 <code class="docutils literal notranslate"><span class="pre">fill-in-the-black</span> <span class="pre">task</span></code>로 주변의 단어들을 사용해서 blank(masked word)의 원래 word를 유추해 내는 것이고, deberta의 차이는 MLM을 하되 위의 disentangled 어텐션을 사용해서 두 개의 별개의 vector를 이용해서 MLM을 수행한다. 문제는 position embedding이 상대적인 위치를 사용하지만 단어의 절대적 위치가 없다는 것이다. 절대적 위치는 구문론적으로 이해를 돕는 굉장히 중요한 정보라고 말한다.</p>
<blockquote>
<div><p>a new store opened beside the new mall</p>
</div></blockquote>
<p>이러한 예시에서 store, mall이 둘 다 masking되었을때, 둘의 context적인 의미는 ‘가게’로 비슷하지만 구문론적으로 앞의 store는 주어의 역할을 하고 있다. 이러한 구문론적인 정보를 주는 것은 context, content가 아니라 absolute position이다.</p>
<p>해서 deberta에서는 이러한 absolute position embedding을 softmax layer 직전에 넣어준다. 바로 이 지점이 모델이 앞에서 말한 content, position embedding 조합을 기반으로 masking을 해독하기 직전이다. 그니깐 거의 마지막에 넣어주는 것으로 ‘참조’의 수준으로 하는 것으로 보인다.</p>
</section>
<section id="virtual-adversarial-training">
<h3>1.3 virtual adversarial training<a class="headerlink" href="#virtual-adversarial-training" title="Permalink to this heading">#</a></h3>
<p>이건 자세하게 설명이 앞에 안나오는데 model generalization에 좋다고 한다.</p>
</section>
<section id="mlm">
<h3>1.4 MLM<a class="headerlink" href="#mlm" title="Permalink to this heading">#</a></h3>
<p>Masked Language Model</p>
<p>기존의 large-scale transformer-based PLM(pretrained language model)들은 보통 많은 양의 텍스트를 가지고 문맥적인 단어의 표현(contextual word representation)을 배우기 위해서 학습 과정을 거쳤다. 이 학습 과정은 ‘self-supervision objective’ 그러니깐 자기지도 학습인데, 방법론 적으로는 MLM을 가리킨다. 이제 수식적으로 들어가보자</p>
<div class="dropdown note admonition">
<p class="admonition-title">MLM</p>
<p>given a sequence <span class="math notranslate nohighlight">\(X = \{x_i\}\)</span><br />
corrupt it into <span class="math notranslate nohighlight">\(\tilde{X}\)</span> by 15% of its tokens at random<br />
model parameterized by <span class="math notranslate nohighlight">\(\theta\)</span><br />
train model to reconstruct <span class="math notranslate nohighlight">\(\tilde{X}\)</span><br />
by predicting the masked token <span class="math notranslate nohighlight">\(\tilde{x}\)</span> conditioned on <span class="math notranslate nohighlight">\(\tilde{X}\)</span></p>
<div class="math notranslate nohighlight">
\[
\max_{\theta}\log p_{\theta}(X|\tilde{X}) = \max_{\theta} \sum_{i\in C} \log p_{\theta}(x_i = x_i | \tilde{X})
\]</div>
<ul class="simple">
<li><p>C = index set of the masked tokens in sequence</p></li>
<li><p>10%의 masked tokens는 변치않은 대로 두고</p></li>
<li><p>다른 10%는 무작위로 고른 것과 바꾸고</p></li>
<li><p>80%는 mask token으로 바꾸라고 한다.</p></li>
</ul>
</div>
</section>
</section>
<section id="deberta-architecture">
<h2>DeBERTa architecture<a class="headerlink" href="#deberta-architecture" title="Permalink to this heading">#</a></h2>
<section id="id1">
<h3>3.1 Disentangled attention<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<p>a two-vector approach to content and position embedding</p>
<blockquote>
<div><p>$i<span class="math notranslate nohighlight">\( : position of token in a sequence\
\)</span>{H_i}<span class="math notranslate nohighlight">\( : content of token, hidden state, output from encoding\
\)</span>{P_{i|j}}$ : relative position with j-th token, from distance of tokens\</p>
</div></blockquote>
<p><strong>cross attention score token_i / token_j</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{matrix}
A_{i,j} &amp;=&amp; \{H_i,P_{i|j}\} \times \{H_i,P_{j|i}\}^{\intercal}\\
        &amp;=&amp; H_iH_j^{\intercal} + H_iP_{j|i}^{\intercal} + P_{i|j}H_j^{\intercal}+P_{i|j}P_{j|i}^{\intercal}\\
\end{matrix}
\end{split}\]</div>
<p>하나의 단어 쌍에 대해서 attention weight를 계산하려면 4 attention score를 구해야 한다. 이 과정에서 사용되는 것이 <code class="docutils literal notranslate"><span class="pre">disentangled</span> <span class="pre">matrices</span></code>이고 이는 content position 두 개에 대해서 <code class="docutils literal notranslate"><span class="pre">content-content</span></code>,<code class="docutils literal notranslate"><span class="pre">content-position</span></code>,<code class="docutils literal notranslate"><span class="pre">position-content</span></code>,<code class="docutils literal notranslate"><span class="pre">position-position</span></code>로 4개가 된다.</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(H_i H_j^{\intercal}\)</span>: i번째 토큰의 내용 정보와 j번째 토큰의 내용 정보 간의 상호작용을 나타냅니다. 이는 토큰 간의 내용적인 유사성을 측정합니다.<code class="docutils literal notranslate"><span class="pre">content-content</span></code></p></li>
<li><p><span class="math notranslate nohighlight">\(H_i P_{j|i}^{\intercal}\)</span>: i번째 토큰의 내용 정보와 j번째 토큰에 대한 i번째 토큰의 상대적 위치 정보 간의 상호작용을 나타냅니다. 이는 i번째 토큰의 내용이 j번째 토큰에 대한 위치에 어떻게 영향을 미치는지를 측정합니다.<code class="docutils literal notranslate"><span class="pre">content-position</span></code></p></li>
<li><p><span class="math notranslate nohighlight">\(P_{i|j} H_j^{\intercal}\)</span>: i번째 토큰에 대한 j번째 토큰의 상대적 위치 정보와 j번째 토큰의 내용 정보 간의 상호작용을 나타냅니다. 이는 j번째 토큰의 내용이 i번째 토큰에 대한 위치에 어떻게 영향을 미치는지를 측정합니다.<code class="docutils literal notranslate"><span class="pre">position-content</span></code></p></li>
<li><p><span class="math notranslate nohighlight">\(P_{i|j} P_{j|i}^{\intercal}\)</span>: i번째 토큰에 대한 j번째 토큰의 상대적 위치 정보와 j번째 토큰에 대한 i번째 토큰의 상대적 위치 정보 간의 상호작용을 나타냅니다. 이는 토큰 간의 상대적인 위치적 유사성을 측정합니다.<code class="docutils literal notranslate"><span class="pre">position-position</span> <span class="pre">-</span> <span class="pre">relative</span> <span class="pre">position</span> <span class="pre">embedding에서는</span> <span class="pre">제거되는</span> <span class="pre">부분</span></code></p></li>
</ol>
<p><strong>이 4개를 disentangled attentions라고 하는 것이다</strong><br />
이전의 relative position encoding은 기존의 attention weight에다가 relative position bias를 더해주는 식으로 위에서의 1,2번만을 사용하는 식으로 사용되었다. 하지만 deberta에서는 이렇게 모든 것을 사용하되 4번은 안사용한다. 이미 1,2,3에서 두 토큰 간의 상대적 위치 정보를 충분히 capture한다고 판단했기 때문에 수식상으로는 out되었다고 한다.</p>
</section>
<section id="standard-self-attention-operation">
<h3>3.1.1 standard self-attention operation<a class="headerlink" href="#standard-self-attention-operation" title="Permalink to this heading">#</a></h3>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(R^{N \times d}\)</span> : N(문장의 길이, 토큰의 수)개의 행 * d(토큰의 hidden 벡터의 차원)개의 열 - 차원의 실수® 행렬<br />
<span class="math notranslate nohighlight">\(H \in R^{N \times d}\)</span> : input hidden vectors<br />
<span class="math notranslate nohighlight">\(H_o \in R^{N \times d}\)</span> : output of self-attention<br />
<span class="math notranslate nohighlight">\(W_q,W_k,W_v \in R^{d \times d}\)</span> : projection matrices<br />
<span class="math notranslate nohighlight">\(A \in R^{N \times N}\)</span> : attention matrix</p>
</div></blockquote>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{matrix}
Q &amp;=&amp; HW_q,\\
K &amp;=&amp; HW_K,\\
V &amp;=&amp; HW_v,\\
A &amp;=&amp; \frac{QK^T}{\sqrt{d}}\\
H_o &amp;=&amp; \text{softmax}(A)V\\
\end{matrix}
\end{split}\]</div>
<p>여기서 Q, K, V는 각각 query, key, value를 나타내며, d_k는 key의 차원을 나타냅니다. 이 때 각 토큰의 위치 정보는 absolute positional encoding 방식을 통해 각 토큰의 hidden state에 추가되고, 이 정보가 포함된 hidden state가 query, key, value로 사용됩니다.</p>
</section>
<section id="relative-distance">
<h3>3.1.2 relative distance<a class="headerlink" href="#relative-distance" title="Permalink to this heading">#</a></h3>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(k\)</span> : maximum relative distance<br />
<span class="math notranslate nohighlight">\(\delta(i,j)\in[0,2k)\)</span> : the relative distance from token i to token j</p>
</div></blockquote>
<div class="math notranslate nohighlight">
\[\begin{split}
\delta(i,j) =
\begin{cases}
0 &amp; \text{for} &amp; i-j\leqslant-k\\
2k-1 &amp; \text{for} &amp; i-j\geqslant k\\
i-j+k &amp;\text{others.} &amp;\\
\end{cases}
\end{split}\]</div>
<p>여기서 [ : 포함, ) : 미포함을 의미한다. 상대적 거리값이 0&lt;=x&lt;2k 의 뜻이라고 한다. 0~2k-1사이의 정수 값</p>
</section>
<section id="disentangled-self-attention">
<h3>3.1.3 disentangled self-attention<a class="headerlink" href="#disentangled-self-attention" title="Permalink to this heading">#</a></h3>
<p>with relative position bias as equation 4</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(Q_c,K_c,V_c\)</span> : projected content vectors from projection matrices<br />
<span class="math notranslate nohighlight">\(W_{q,c},W_{k,c},W_{v,c} \in R^{d\times d}\)</span> : projection matrices<br />
<span class="math notranslate nohighlight">\(P \in R^{2k\times d}\)</span> : relative position embedding vectors shared with all layers<br />
<span class="math notranslate nohighlight">\(Q_r, K_r\)</span> : projected relative position vectors<br />
<span class="math notranslate nohighlight">\(W_{q,r},W_{k,r} \in R^{d\times d}\)</span> : projection matrices</p>
</div></blockquote>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{matrix}
Q_c &amp;=&amp; HW_{q,c},\\
K_c &amp;=&amp; HW_{k,c},\\
V_c &amp;=&amp; HW_{v,c},\\
Q_r &amp;=&amp; PW_{q,r},\\
K_r &amp;=&amp; PW_{k,r}\\
\end{matrix}
\end{split}\]</div>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(\tilde{A}_{i,j}\)</span> : <span class="math notranslate nohighlight">\(\tilde{A}\)</span>의 요소. 토큰 i에서 token j 로의 어텐션 스코어<br />
<span class="math notranslate nohighlight">\(Q_i^c\)</span> : i-th row of <span class="math notranslate nohighlight">\(Q_c\)</span><br />
<span class="math notranslate nohighlight">\(Q^r_{\delta(j,i)}\)</span> : <span class="math notranslate nohighlight">\(Q_r\)</span> with regarding to 상대적 거리 <span class="math notranslate nohighlight">\(\delta(j,i)\)</span></p>
</div></blockquote>
<div class="math notranslate nohighlight">
\[
\tilde{A}_{i,j} = Q_i^cK_j^{c\intercal} + Q_i^c{K_{\delta(i,j)}^r}^\intercal + K_j^c{Q_{\delta(j,i)^r}}^\intercal
\]</div>
</section>
<section id="projection-matrix">
<h3>projection matrix<a class="headerlink" href="#projection-matrix" title="Permalink to this heading">#</a></h3>
<p>한 벡터나 공간을 다른 공간으로 mapping하는 것을 도우는 matrix</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./scripts\paper_review"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>




                <footer class="bd-footer-article">

<div class="footer-article-items footer-article__inner">

    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="DebertaV3.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Deberta V3</p>
      </div>
    </a>
    <a class="right-next"
       href="../algorithm/intro.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">intro</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>

</div>

                </footer>

            </div>



                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">문제 설정 Introduction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parallelization">1.1 Parallelization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#disentangled-attention">1.2 Disentangled attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#enhanced-mask-decoder">1.3 Enhanced mask decoder</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#virtual-adversarial-training">1.3 virtual adversarial training</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mlm">1.4 MLM</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deberta-architecture">DeBERTa architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">3.1 Disentangled attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#standard-self-attention-operation">3.1.1 standard self-attention operation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#relative-distance">3.1.2 relative distance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#disentangled-self-attention">3.1.3 disentangled self-attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#projection-matrix">projection matrix</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>


          </div>
          <footer class="bd-footer-content">

<div class="bd-footer-content__inner container">

  <div class="footer-item">

<p class="component-author">
By Surdarla
</p>

  </div>

  <div class="footer-item">

  <p class="copyright">

      © Copyright 2023.
      <br/>

  </p>

  </div>

  <div class="footer-item">

  </div>

  <div class="footer-item">

  </div>

</div>
          </footer>


      </main>
    </div>
  </div>

  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>
