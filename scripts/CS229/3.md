# Locally Weighted & Logistic Regression

## Locally weighted regression

```{admonition} underfitting, overfitthing
이전의 linear regression에서 우리는 $h_{\theta}(x) = \theta_{0} + \theta_{1}x$를 찾아가는 과정을 보았다. 이것은 **underfitting**이라고 볼 수 있다. 이것은 데이터의 특성을 잘 반영하지 못한다. 반대로 **overfitting**은 데이터의 특성을 너무 잘 반영해서 noise까지 학습하는 것을 말한다. 이러한 경우는 결국 예측을 잘하는 것처럼 보이긴하고 점수적으로는 좋을 수 있으나, 사실상 새로운 test set에 대해서는 점수가 안나오게 된다. 결국 $h(x)$ 우리의 가설함수가 잘`못된 경우라고 볼 수 있다.
```

LWR은 이러한 문제를 해결하기 위해 사용되는 방법이다. LWR은 다음과 같은 가설함수를 사용한다.
Fit $\theta_i$ to minimize where $w_i$ is a "weighted" function


### parametric/non-parametric learning algorithm

Parametric learning algorithm은 fit fixed set of parameter($\theta_i$) to data인 경우를 말한다. 즉 hypothesis의 구해야 하는 파라미터가 정해진 경우를 말한다. Linear regression, Logistic regression, Bayesian inference, Neural network(CNN, RNN 등) 등. 모델이 학습해야 하는 것이 명확히 정해져 있기 때문에 속도가 빠르고, 모델을 이해하기가 쉽다는 장점이 있다. 하지만 데이터의 분포가 특정한 분포를 따른다는 가정을 해야 하기 때문에 flexibility가 낮고, 간단한 문제를 푸는 데에 더 적합하다는 단점을 가진다.

반대로 Non-parametric learning algorithm은 구해야하는 파라미터가 지정되지 않는 경우를 말한다. Amount of data/parameters you need to keep grow(linearly) with size of data - 구해야하는 파라미터는 데이터의 크기에 따라 선형적으로 커진다. Decision tree, Random forest, K-nearest neighbor classifier 등. 데이터가 특정한 분포를 따른다는 가정을 하지 않기 때문에 더 flexible하다는 장점이 있다. 하지만 속도가 느린 경우가 많고, 더 큰 데이터를 필요로 하는 경우가 있으며 모델이 왜 그런 형태가 되었는지에 대한 명확한 설명을 하기가 쉽지 않다. 위에서의 LWR는 non-parametric learning algorithm에 속한다.

## Logistic Regression(sigmoid function)

$$
h_\theta(x) = g(\theta^T x) = 1+e^{-\theta^T x} \\
where \\
g(z) = 1+e^{-z}
$$

g(z)는 1에서 $z \to \infin$ and g(z) 0으로 간다 $z \to -\infin$

$\theta^T x = \theta_0 + \sum{n}{j=1}\theta_j x_j$

## Reference

1. [Blog - parametric model과 Non-parametric model](https://process-mining.tistory.com/131)
