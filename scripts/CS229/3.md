# Locally Weighted & Logistic Regression

## Locally weighted regression

```{admonition} underfitting, overfitthing
ì´ì „ì˜ linear regressionì—ì„œ ìš°ë¦¬ëŠ” $h_{\theta}(x) = \theta_{0} + \theta_{1}x$ë¥¼ ì°¾ì•„ê°€ëŠ” ê³¼ì •ì„ ë³´ì•˜ë‹¤. ì´ê²ƒì€ **underfitting**ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ì´ê²ƒì€ ë°ì´í„°ì˜ íŠ¹ì„±ì„ ì˜ ë°˜ì˜í•˜ì§€ ëª»í•œë‹¤. ë°˜ëŒ€ë¡œ **overfitting**ì€ ë°ì´í„°ì˜ íŠ¹ì„±ì„ ë„ˆë¬´ ì˜ ë°˜ì˜í•´ì„œ noiseê¹Œì§€ í•™ìŠµí•˜ëŠ” ê²ƒì„ ë§í•œë‹¤. ì´ëŸ¬í•œ ê²½ìš°ëŠ” ê²°êµ­ ì˜ˆì¸¡ì„ ì˜í•˜ëŠ” ê²ƒì²˜ëŸ¼ ë³´ì´ê¸´í•˜ê³  ì ìˆ˜ì ìœ¼ë¡œëŠ” ì¢‹ì„ ìˆ˜ ìˆìœ¼ë‚˜, ì‚¬ì‹¤ìƒ ìƒˆë¡œìš´ test setì— ëŒ€í•´ì„œëŠ” ì ìˆ˜ê°€ ì•ˆë‚˜ì˜¤ê²Œ ëœë‹¤. ê²°êµ­ $h(x)$ ìš°ë¦¬ì˜ ê°€ì„¤í•¨ìˆ˜ê°€ ì˜`ëª»ëœ ê²½ìš°ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.
```

LWRì€ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ë°©ë²•ì´ë‹¤. LWRì€ ë‹¤ìŒê³¼ ê°™ì€ ê°€ì„¤í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤.
Fit $\theta_i$ to minimize where $w_i$ is a "weighted" function

### parametric/non-parametric learning algorithm

Parametric learning algorithmì€ fit fixed set of parameter($\theta_i$) to dataì¸ ê²½ìš°ë¥¼ ë§í•œë‹¤. ì¦‰ hypothesisì˜ êµ¬í•´ì•¼ í•˜ëŠ” íŒŒë¼ë¯¸í„°ê°€ ì •í•´ì§„ ê²½ìš°ë¥¼ ë§í•œë‹¤. Linear regression, Logistic regression, Bayesian inference, Neural network(CNN, RNN ë“±) ë“±. ëª¨ë¸ì´ í•™ìŠµí•´ì•¼ í•˜ëŠ” ê²ƒì´ ëª…í™•íˆ ì •í•´ì ¸ ìˆê¸° ë•Œë¬¸ì— ì†ë„ê°€ ë¹ ë¥´ê³ , ëª¨ë¸ì„ ì´í•´í•˜ê¸°ê°€ ì‰½ë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤. í•˜ì§€ë§Œ ë°ì´í„°ì˜ ë¶„í¬ê°€ íŠ¹ì •í•œ ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ëŠ” ê°€ì •ì„ í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— flexibilityê°€ ë‚®ê³ , ê°„ë‹¨í•œ ë¬¸ì œë¥¼ í‘¸ëŠ” ë°ì— ë” ì í•©í•˜ë‹¤ëŠ” ë‹¨ì ì„ ê°€ì§„ë‹¤.

ë°˜ëŒ€ë¡œ Non-parametric learning algorithmì€ êµ¬í•´ì•¼í•˜ëŠ” íŒŒë¼ë¯¸í„°ê°€ ì§€ì •ë˜ì§€ ì•ŠëŠ” ê²½ìš°ë¥¼ ë§í•œë‹¤. Amount of data/parameters you need to keep grow(linearly) with size of data - êµ¬í•´ì•¼í•˜ëŠ” íŒŒë¼ë¯¸í„°ëŠ” ë°ì´í„°ì˜ í¬ê¸°ì— ë”°ë¼ ì„ í˜•ì ìœ¼ë¡œ ì»¤ì§„ë‹¤. Decision tree, Random forest, K-nearest neighbor classifier ë“±. ë°ì´í„°ê°€ íŠ¹ì •í•œ ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ëŠ” ê°€ì •ì„ í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ë” flexibleí•˜ë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤. í•˜ì§€ë§Œ ì†ë„ê°€ ëŠë¦° ê²½ìš°ê°€ ë§ê³ , ë” í° ë°ì´í„°ë¥¼ í•„ìš”ë¡œ í•˜ëŠ” ê²½ìš°ê°€ ìˆìœ¼ë©° ëª¨ë¸ì´ ì™œ ê·¸ëŸ° í˜•íƒœê°€ ë˜ì—ˆëŠ”ì§€ì— ëŒ€í•œ ëª…í™•í•œ ì„¤ëª…ì„ í•˜ê¸°ê°€ ì‰½ì§€ ì•Šë‹¤. ìœ„ì—ì„œì˜ LWRëŠ” non-parametric learning algorithmì— ì†í•œë‹¤.

## Logistic Regression(sigmoid function)

$$
h_\theta(x) = g(\theta^T x) = 1+e^{-\theta^T x} \\
where \\
g(z) = 1+e^{-z}
$$

g(z)ëŠ” 1ì—ì„œ $z \to \infin$ and g(z) 0ìœ¼ë¡œ ê°„ë‹¤ $z \to -\infin$

$\theta^T x = \theta_0 + \sum{n}{j=1}\theta_j x_j$

## Reference

1. [Blog - parametric modelê³¼ Non-parametric model](https://process-mining.tistory.com/131)


```{raw} html
<script
   type="text/javascript"
   src="https://utteranc.es/client.js"
   async="async"
   repo="surdarla/surdarla.github.io"
   issue-term="pathname"
   theme="github-light"
   label="ğŸ’¬ comment"
   crossorigin="anonymous"
/>
```
