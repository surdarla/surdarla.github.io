

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>DeBERTa &#8212; surdarla-book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=365ca57ee442770a23c6" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/my_css.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6" />
  <script src="../../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=365ca57ee442770a23c6"></script>

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script kind="utterances">

    var commentsRunWhenDOMLoaded = cb => {
    if (document.readyState != 'loading') {
        cb()
    } else if (document.addEventListener) {
        document.addEventListener('DOMContentLoaded', cb)
    } else {
        document.attachEvent('onreadystatechange', function() {
        if (document.readyState == 'complete') cb()
        })
    }
}

var addUtterances = () => {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "https://utteranc.es/client.js";
    script.async = "async";

    script.setAttribute("repo", "surdarla/surdarla.github.io");
    script.setAttribute("issue-term", "pathname");
    script.setAttribute("theme", "github-light");
    script.setAttribute("label", "ğŸ’¬ comment");
    script.setAttribute("crossorigin", "anonymous");

    sections = document.querySelectorAll("div.section");
    if (sections !== null) {
        section = sections[sections.length-1];
        section.appendChild(script);
    }
}
commentsRunWhenDOMLoaded(addUtterances);
</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-BMRNT7D57Q"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BMRNT7D57Q');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'scripts/paper_review/DebertaV1';</script>
    <link rel="canonical" href="https://surdarla.github.io/scripts/paper_review/DebertaV1.html" />
    <link rel="shortcut icon" href="../../_static/my_favi.png"/>
    <link rel="author" title="About these documents" href="../../about.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="problem-solving" href="../algorithm/problem-solving.html" />
    <link rel="prev" title="Deberta V3" href="DebertaV3.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>


  <div class="bd-header-announcement container-fluid bd-header-announcement">
    <div class="bd-header-announcement__content">Refactoing Ongoing ...</div>
  </div>

  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../about.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/surdarla-logo_old.png" class="logo__image only-light" alt="surdarla-book - Home"/>
    <script>document.write(`<img src="../../_static/surdarla-logo_old.png" class="logo__image only-dark" alt="surdarla-book - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../about.html">
                    Surdarla ì…ë‹ˆë‹¤
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">CS229 summary</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../CS229/intro.html">intro</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../CS229/1.html">Learning Algorithms</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../CS229/2.html">Linear Regressin &amp; Gradient Descent</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../CS229/2-1.html">practice - torch sklearn numpy</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Basics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../basics/anal/anal_intro.html">Analysis</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../basics/anal/linearity.html">linearity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../basics/anal/information-value.html">Feature selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../basics/anal/EDA.html">EDA(Exploratory Data Analysis) íƒìƒ‰ì  ë¶„ì„</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../basics/python/python_intro.html">python</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../basics/python/poetry.html">Poetry ì‚¬ìš©í•˜ê¸°</a></li>
<li class="toctree-l2"><a class="reference internal" href="../basics/python/pandas.html">join merge</a></li>

<li class="toctree-l2"><a class="reference internal" href="../basics/python/OOP.html">ê°ì²´ ì§€í–¥ í”„ë¡œê·¸ë˜ë°</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../basics/dl/DL_intro.html">Deep learning</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../basics/dl/tokenizing.html">ì»´í“¨í„°ì˜ ì–¸ì–´í‘œí˜„</a></li>
<li class="toctree-l2"><a class="reference internal" href="../basics/dl/NLP_basics.html">Normalization ì •ê·œí™”</a></li>
<li class="toctree-l2"><a class="reference internal" href="../basics/dl/perceptron.html">Perceptron</a></li>
<li class="toctree-l2"><a class="reference internal" href="../basics/dl/attention.html">Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="../basics/dl/softmax.html">softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="../basics/dl/Logistic.html">Logistic</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../basics/js/JS.html">Javascript</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../basics/js/VUE.html">vue</a></li>
<li class="toctree-l2"><a class="reference internal" href="../basics/js/DOM.html">DOM</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../basics/tips/tips_intro.html">Tips</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../basics/tips/markdownlint.html">vscode markdownlint diableing MD~</a></li>
<li class="toctree-l2"><a class="reference internal" href="../basics/tips/snippet_create.html">vscode snippet ë§Œë“¤ê¸°</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../basics/java/java_intro.html">Java</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../basics/java/var_type.html">ë³€ìˆ˜ì™€ ìë£Œí˜•</a></li>
<li class="toctree-l2"><a class="reference internal" href="../basics/java/control_state.html">ì œì–´ë¬¸</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Paper Review</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="vision.html">vision</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="resnet.html">Resnet</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="nlp.html">nlp</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="DebertaV3.html">Deberta V3</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">DeBERTa</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Algorithm</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../algorithm/problem-solving.html">problem-solving</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/2558.html">2558 : A + B - 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/2530.html">2530 : ì¸ê³µì§€ëŠ¥ ì‹œê³„</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/2117.html">2117 : ì›í˜• ëŒ„ìŠ¤</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/3046.html">3046 : R2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/2164.html">2164 : ì¹´ë“œ2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/2525.html">2525 : ì˜¤ë¸ ì‹œê³„</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/1012.html">1012 : ìœ ê¸°ë† ë°°ì¶”</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/%EC%98%B9%EC%95%8C%EC%9D%B4%281%29.html">ì˜¹ì•Œì´(1)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/%EA%B0%99%EC%9D%80%EC%88%AB%EC%9E%90%EB%8A%94%EC%8B%AB%EC%96%B4.html">ê°™ì€ ìˆ«ìëŠ” ì‹«ì–´</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/%EC%95%88%EC%A0%84%EC%A7%80%EB%8C%80.html">ì•ˆì „ì§€ëŒ€</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/%EA%B3%B5%EB%8D%98%EC%A7%80%EA%B8%B0.html">ê³µë˜ì§€ê¸°</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/%ED%95%98%EB%85%B8%EC%9D%B4%EC%9D%98%20%ED%83%91.html">í•˜ë…¸ì´ì˜ íƒ‘</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/%EC%9D%98%EC%83%81.html">ì˜ìƒ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/10825.html">10825 : êµ­ì˜ìˆ˜</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/23300.html">23300 : ì›¹ ë¸Œë¼ìš°ì € 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/%EC%98%AC%EB%B0%94%EB%A5%B8%EA%B4%84%ED%98%B8.html">ì˜¬ë°”ë¥¸ ê´„í˜¸</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/10816.html">10816 : ìˆ«ì ì¹´ë“œ 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/21921.html">21921 : ë¸”ë¡œê·¸</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/1535.html">1535 : ì•ˆë…•</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/9663.html">9663 : N-Queen</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/1006.html">1006 : ìŠµê²©ì ì´ˆë¼ê¸°</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/24465.html">24465 : ë°ë·”ì˜ ê¿ˆ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/1065.html">1065 : í•œìˆ˜</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/17952.html">17952 : ê³¼ì œëŠ” ëë‚˜ì§€ ì•Šì•„!</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/problem/20207.html">20207 : ë‹¬ë ¥</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../algorithm/data_structure.html">data_structure</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/%EC%9E%90%EB%A3%8C%EA%B5%AC%EC%A1%B0.html">ìë£Œêµ¬ì¡°</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/type_in_python.html">Type in python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/%EC%B5%9C%EC%86%8C%EB%B9%84%EC%9A%A9%EA%B2%BD%EB%A1%9C.html">ìµœì†Œë¹„ìš©ê²½ë¡œ ë¬¸ì œ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/%EC%B5%9C%EB%8C%80%EC%9C%A0%EB%9F%89.html">Network Flow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/BFS.html">BFS</a></li>
<li class="toctree-l2"><a class="reference internal" href="../algorithm/DP.html">Dynamic Programming</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Certificate</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../certificate/SQLD/SQL_intro.html">SQLD ì¤€ë¹„ê³¼ì •</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../certificate/SQLD/1.html">1. ë°ì´í„° ëª¨ë¸ë§ì˜ ì´í•´</a></li>
<li class="toctree-l2"><a class="reference internal" href="../certificate/SQLD/2.html">2. ë°ì´í„° ëª¨ë¸ê³¼ ì„±ëŠ¥</a></li>
<li class="toctree-l2"><a class="reference internal" href="../certificate/SQLD/3.html">3. SQL ê¸°ë³¸</a></li>
<li class="toctree-l2"><a class="reference internal" href="../certificate/SQLD/4.html">4. SQL í™œìš©</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../certificate/Big_data/big_intro.html">ë¹…ë¶„ê¸° ì¤€ë¹„ê³¼ì •</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../certificate/Big_data/1.html">1. ë¹…ë°ì´í„° ë¶„ì„ ê¸°íš</a></li>
<li class="toctree-l2"><a class="reference internal" href="../certificate/Big_data/2.html">2. ë¹…ë°ì´í„° íƒìƒ‰</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/surdarla/surdarla.github.io" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/surdarla/surdarla.github.io/issues/new?title=Issue%20on%20page%20%2Fscripts/paper_review/DebertaV1.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/scripts/paper_review/DebertaV1.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>DeBERTa</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">ë¬¸ì œ ì„¤ì • Introduction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parallelization">1.1 Parallelization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#disentangled-attention">1.2 Disentangled attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#enhanced-mask-decoder">1.3 Enhanced mask decoder</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#virtual-adversarial-training">1.3 virtual adversarial training</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mlm">1.4 MLM</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deberta-architecture">DeBERTa architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">3.1 Disentangled attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#standard-self-attention-operation">3.1.1 standard self-attention operation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#relative-distance">3.1.2 relative distance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#disentangled-self-attention">3.1.3 disentangled self-attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#projection-matrix">projection matrix</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="deberta">
<h1>DeBERTa<a class="headerlink" href="#deberta" title="Permalink to this heading">#</a></h1>
<p>DEBERTA: DECODING-ENHANCED BERT WITH DIS-ENTANGLED ATTENTION</p>
<p>Published as a conference paper at ICLR 2023 <a class="sd-sphinx-override sd-badge sd-bg-primary sd-bg-text-primary reference external" href="https://arxiv.org/pdf/2006.03654"><span>Paper PDF</span></a></p>
<p>Pengcheng He, Microsoft Dynamics 365 AI<br />
Xiaodong Liu, Microsoft Research<br />
Jianfeng Gao, Microsoft Research<br />
Weizhu Chen, Microsoft Dynamics 365 AI<br />
{penhe,xiaodl,jfgao,wzchen}&#64;microsoft.com</p>
<p>microsoftì—ì„œ debertaë¥¼ ë§Œë“¤ì—ˆë‹¤ë³´ë‹ˆ ì§‘í•„ì§„ì´ ì „ë¶€ microsoftì´ë‹¤.</p>
<div class="dropdown note admonition">
<p class="admonition-title">Abstract</p>
<p>Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is <strong>the disentangled attention mechanism</strong>, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an <strong>enhanced mask decoder</strong> is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a <strong>new virtual adversarial training method</strong> is used for fine-tuning to improve modelsâ€™ generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understanding (NLU) and natural langauge generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). Notably, we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters. The significant performance boost makes the single DeBERTa model surpass the human performance on the SuperGLUE benchmark (Wang et al., 2019a) for the first time in terms of macro-average score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the SuperGLUE leaderboard as of January 6, 2021, out performing the human baseline by a decent margin (90.3 versus 89.8).</p>
</div>
<section id="introduction">
<h2>ë¬¸ì œ ì„¤ì • Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">#</a></h2>
<p>TransformerëŠ” nlpì—ì„œ ê°€ì¥ íš¨ê³¼ì ì¸ network architectureë¡œ ìë¦¬ì¡ì•˜ë‹¤. ìˆœì„œì— ë”°ë¼ textë¥¼ ì²˜ë¦¬í•˜ëŠ” RNNë“¤ê³¼ ë‹¤ë¥´ê²Œ, tranformersëŠ” ì…ë ¥ í…ìŠ¤íŠ¸(input text)ì˜ ëª¨ë“  ë‹¨ì–´(every words)ì— ëŒ€í•´ì„œ self-attentionì„ ì ìš©í•˜ì—¬, attention weightë¥¼ ë½‘ì•„ë‚¸ë‹¤. ê·¸ë¦¬ê³  ì´ attention weightëŠ” ê° ë‹¨ì–´ë“¤ì´ ì„œë¡œì—ê²Œ ì–´ë– í•œ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìˆ˜ì¹˜ì´ë‹¤. ì´ëŸ¬í•œ ë°©ì‹(ë³‘ë ¬ì²˜ë¦¬)ì„ transformerê°€ ì‚¬ìš©í•˜ê³  ìˆê¸° ë•Œë¬¸ì— RNNê³¼ ê°™ì€ ìˆœì°¨ì²˜ë¦¬ ëª¨ë¸ë³´ë‹¤ large-scale trainingì´ ê°€ëŠ¥í•œ ê²ƒì´ë‹¤.</p>
<section id="parallelization">
<h3>1.1 Parallelization<a class="headerlink" href="#parallelization" title="Permalink to this heading">#</a></h3>
<p>RNNê³¼ transformerëŠ” ë‘˜ ë‹¤ sequence modeling architectureì´ë‹¤. í•˜ì§€ë§Œ transformerëŠ” ì‹œí€€ìŠ¤ì˜ ëª¨ë“  ìœ„ì¹˜ì— ëŒ€í•´ ë™ì‹œì— ì—°ì‚°ì„ ìˆ˜í–‰í•´ì„œ ë³‘ë ¬í™”ê°€ ë” ì‰½ë‹¤. ë³‘ë ¬í™”ë¼ëŠ” ë§ ìì²´ëŠ” í•˜ë‚˜ì˜ ì‘ì—…ì„ ì—¬ëŸ¬ ê°œì˜ ì‘ì€ ìê²…ã…‚ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë™ì‹œì— ì²˜ë¦¬í•¨ìœ¼ë¡œì¨ ì „ì²´ì ìœ¼ë¡œëŠ” í•˜ë‚˜ì˜ ì‘ì—…ì„ ë” ë¹ ë¥´ê³  íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ê¸°ìˆ ì„ ë§í•œë‹¤. ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ í•™ìŠµì€ ê³„ì‚°ëŸ‰, ë°ì´í„°ëŸ‰ì´ ë§ì„ ìˆ˜ë¡ ì¢‹ì€ ë°©í–¥ìœ¼ë¡œ í˜ëŸ¬ê°€ê³  ìˆê¸° ë•Œë¬¸ì— ì´ëŸ¬í•œ ê¸°ìˆ ì€ í•„ìˆ˜ì ì´ë‹¤. GPUë¥¼ ë™ì‹œì— ì‚¬ìš©í•˜ê±°ë‚˜ ë°ì´í„°ë¥¼ ìª¼ê°œê±°ë‚˜, ì—°ì‚°ì„ ìª¼ê°œê±°ë‚˜ í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì—¬ëŸ¬ ë¶€ë¶„ì—ì„œ ìª¼ê°œì–´ ë™ì‹œë‹¤ë°œì ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ê²ƒì„ ì „ë°˜ì ìœ¼ë¡œ ë³‘ë ¬í™”ë¼ê³  ë§í•  ìˆ˜ ìˆë‹¤.</p>
<p>ë³‘ë ¬í™”ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” 3ê°€ì§€ íŠ¹ì§•ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>
<ol class="arabic simple">
<li><p>Attention ë©”ì»¤ë‹ˆì¦˜: TransformerëŠ” Self-Attention ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ëª¨ë“  ìš”ì†Œë¥¼ ë™ì‹œì— ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¡œ ì¸í•´ ì„œë¡œ ë…ë¦½ì ì¸ ê³„ì‚°ì´ ê°€ëŠ¥í•˜ë©°, ë³‘ë ¬ ì²˜ë¦¬ì— ì í•©í•©ë‹ˆë‹¤.</p></li>
<li><p>Layer ë‹¨ìœ„ ë³‘ë ¬í™”: Transformerì˜ ì¸ì½”ë”ì™€ ë””ì½”ë”ëŠ” ì—¬ëŸ¬ ì¸µìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ì´ëŸ¬í•œ ì¸µë“¤ì€ ë…ë¦½ì ìœ¼ë¡œ ë³‘ë ¬ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p></li>
<li><p>ë§ˆìŠ¤í‚¹: í•™ìŠµ ì‹œì—ëŠ” í•œ ë²ˆì— í•˜ë‚˜ì˜ ì¶œë ¥ë§Œì„ ìƒì„±í•˜ë„ë¡ ë§ˆìŠ¤í‚¹í•˜ì—¬ ìˆœì°¨ì ìœ¼ë¡œ ê³„ì‚°í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì¶”ë¡ (inference) ë‹¨ê³„ì—ì„œëŠ” ë³‘ë ¬í™”í•˜ì—¬ ë™ì‹œì— ì—¬ëŸ¬ ê°œì˜ ì¶œë ¥ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p></li>
</ol>
</section>
<section id="disentangled-attention">
<h3>1.2 Disentangled attention<a class="headerlink" href="#disentangled-attention" title="Permalink to this heading">#</a></h3>
<blockquote>
<div><p>relative position embeddingë¥¼ ì‚¬ìš©í•˜ì. ê·¼ë° vectorë¥¼ ë‘ ê°œë¡œ ë‚˜ëˆ ì„œâ€¦</p>
</div></blockquote>
<p><strong>ê¸°ì¡´ì˜ self-attentionì˜ ìœ„ì¹˜ì •ë„ encoding ë¬¸ì œ</strong><br />
ì›ë˜(transformers)ì˜ self-attention mechanismì€ ì£¼ì–´ì§„ input sequenceì—ì„œ ëª¨ë“  ìœ„ì¹˜ì˜ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ outputì„ ìƒì„±í•œë‹¤. input sequenceì˜ ëª¨ë“  ìœ„ì¹˜ ê°„ì˜ ê´€ê³„ë¥¼ ë†’ì€ ìˆ˜ì¤€ì˜ ìƒí˜¸ì˜ì¡´ì„±ì„ ê°€ì§„ ì „ì—°ê²° ê·¸ë˜í”„ë¡œ ëª¨ë¸ë§ì„ í•˜ê¸° ë•Œë¬¸ì—, sequenceì˜ ê¸¸ì´ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ ì—°ì‚°ëŸ‰ì´ ì¦ê°€í•˜ê³ , ì¥ê¸°ì˜ì¡´ì„±ì„ í•™ìŠµí•˜ëŠ”ë° ì–´ë ¤ì›€ì´ ì»¤ì§„ë‹¤.</p>
<p><strong>positional bias</strong><br />
ë•Œë¬¸ì— ê¸°ì¡´ì˜ word embeddingì— positional biasë¥¼ ì¶”ê°€í•´ì¤˜ì„œ í•˜ë‚˜ì˜ ë²¡í„°ì—ì„œ contentì™€ positionì˜ ì •ë³´ë¥¼ í¬í•¨í•˜ë„ë¡ í•˜ëŠ” ë°©ì‹ì´ ì‚¬ìš©ë˜ì—ˆë‹¤.</p>
<ul class="simple">
<li><p>absolute position embedding : ë¬¸ì¥ ë‚´ ìœ„ì¹˜ë§Œ ê³ ë ¤</p></li>
<li><p>relative position embedding : ë‹¨ì–´ê°„ ìƒëŒ€ì  ìœ„ì¹˜ë§Œ ê³ ë ¤</p></li>
</ul>
<p>ê¸°ì¡´ BERTë¥¼ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì´ë ‡ê²Œ ë§í•œë‹¤. <code class="docutils literal notranslate"><span class="pre">each</span> <span class="pre">word</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">input</span> <span class="pre">layer</span> <span class="pre">is</span> <span class="pre">represented</span> <span class="pre">using</span> <span class="pre">**a</span> <span class="pre">vector**</span> <span class="pre">which</span> <span class="pre">is</span> <span class="pre">the</span> <span class="pre">sum</span> <span class="pre">of</span> <span class="pre">its</span> <span class="pre">word</span> <span class="pre">(content)</span> <span class="pre">embedding</span> <span class="pre">and</span> <span class="pre">(absolute)position</span> <span class="pre">embedding</span></code>. ë¶„ëª…í•˜ê²Œ absolute position ì •ë³´ê°€ ì¤‘ìš”í•œ ê²ƒì€ ë§ì§€ë§Œ ë’¤ì—ì„œ ë”°ë¥¸ ë°©ë²•ìœ¼ë¡œ ì´ìš©í•˜ë˜, ì—¬ê¸°ì„œëŠ” relative position embeddingì„ ì´ìš©í•˜ì—¬ ìœ„ì¹˜ì •ë³´ë¥¼ inputìœ¼ë¡œ ì‚¬ìš©í•˜ê³ , ì‹¤ì œë¡œ ë§ì€ ì„ ì œì—°êµ¬ë“¤ì—ì„œ ì´ ë°©ë²•ì˜ ì¤‘ìš”ì„±ì„ ë§í•´ì¤¬ë‹¤ê³  í•œë‹¤.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p>â€¦</p></th>
<th class="head text-center"><p>word in input layer</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>transformer</p></td>
<td class="text-center"><p>word itself</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Bert</p></td>
<td class="text-center"><p>vector(word embedding(token) + position embedding)</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>Deberta</p></td>
<td class="text-center"><p>vector(word embedding(token)),vector(absoulte position embedding)</p></td>
</tr>
</tbody>
</table>
<p><strong>ì—¬ê¸°ì„œëŠ” relative word embeddingì„ ì‚¬ìš©</strong><br />
disentangled attentionì€ ì´ë ‡ê²Œ ê°ê°ì˜ embeddingìœ¼ë¡œ wordì— ëŒ€í•œ í‘œí˜„ì„ í•œ ë’¤, ê°ê°ì˜ attention weightë¥¼ <code class="docutils literal notranslate"><span class="pre">disentangled</span> <span class="pre">matrices</span> <span class="pre">=</span> <span class="pre">2ê°œì˜</span> <span class="pre">vectors</span> <span class="pre">-&gt;</span> <span class="pre">2x2</span> <span class="pre">=</span> <span class="pre">4ê°œì˜</span> <span class="pre">matrices</span></code>ë¥¼ ì´ìš©í•´ì„œ ê³„ì‚°í•œë‹¤. ì´ëŠ” ë‹¨ì–´ì˜ ê´€ê³„ë¼ëŠ” ê²ƒì´ ì˜ë¯¸ì  ê´€ê³„ë§Œ ìˆëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ìƒëŒ€ì ì¸ ìœ„ì¹˜ì— ì˜í•´ì„œë„ ì˜í–¥ì„ ë°›ëŠ” ê²ƒì—ì„œ ë‚˜ì˜¨ ê²ƒì´ë‹¤. ë³¸ë¬¸ì˜ ì˜ˆì‹œëŠ” â€˜deepâ€™,â€™learningâ€™ì´ë¼ëŠ” ë‘ ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë¬¸ì¥ì—ì„œ ìˆì„ ë•Œë³´ë‹¤ ì˜†ì— ë¶™ì–´ìˆì„ ë•Œ ì˜ì¡´ì„±ì´ êµ‰ì¥íˆ ë†’ì•„ì§€ëŠ” ê²ƒìœ¼ë¡œ ë“ ë‹¤.</p>
</section>
<section id="enhanced-mask-decoder">
<h3>1.3 Enhanced mask decoder<a class="headerlink" href="#enhanced-mask-decoder" title="Permalink to this heading">#</a></h3>
<p>absolute position embedding ë”í•˜ê¸°</p>
<p>MLM : masked language modelingì€ Bertì—ì„œ ë‚˜ì™”ë˜ ê²ƒìœ¼ë¡œ í•´ë‹¹ ëª¨ë¸ ì—­ì‹œ ì´ ë°©ë²•ì„ ì‚¬ìš©í•´ì„œ ê¸°ë³¸ì ì¸ pre-trainingì„ ì§„í–‰í•œë‹¤. MLM ì´ë¼ëŠ” ê²ƒì€ <code class="docutils literal notranslate"><span class="pre">fill-in-the-black</span> <span class="pre">task</span></code>ë¡œ ì£¼ë³€ì˜ ë‹¨ì–´ë“¤ì„ ì‚¬ìš©í•´ì„œ blank(masked word)ì˜ ì›ë˜ wordë¥¼ ìœ ì¶”í•´ ë‚´ëŠ” ê²ƒì´ê³ , debertaì˜ ì°¨ì´ëŠ” MLMì„ í•˜ë˜ ìœ„ì˜ disentangled ì–´í…ì…˜ì„ ì‚¬ìš©í•´ì„œ ë‘ ê°œì˜ ë³„ê°œì˜ vectorë¥¼ ì´ìš©í•´ì„œ MLMì„ ìˆ˜í–‰í•œë‹¤. ë¬¸ì œëŠ” position embeddingì´ ìƒëŒ€ì ì¸ ìœ„ì¹˜ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ ë‹¨ì–´ì˜ ì ˆëŒ€ì  ìœ„ì¹˜ê°€ ì—†ë‹¤ëŠ” ê²ƒì´ë‹¤. ì ˆëŒ€ì  ìœ„ì¹˜ëŠ” êµ¬ë¬¸ë¡ ì ìœ¼ë¡œ ì´í•´ë¥¼ ë•ëŠ” êµ‰ì¥íˆ ì¤‘ìš”í•œ ì •ë³´ë¼ê³  ë§í•œë‹¤.</p>
<blockquote>
<div><p>a new store opened beside the new mall</p>
</div></blockquote>
<p>ì´ëŸ¬í•œ ì˜ˆì‹œì—ì„œ store, mallì´ ë‘˜ ë‹¤ maskingë˜ì—ˆì„ë•Œ, ë‘˜ì˜ contextì ì¸ ì˜ë¯¸ëŠ” â€˜ê°€ê²Œâ€™ë¡œ ë¹„ìŠ·í•˜ì§€ë§Œ êµ¬ë¬¸ë¡ ì ìœ¼ë¡œ ì•ì˜ storeëŠ” ì£¼ì–´ì˜ ì—­í• ì„ í•˜ê³  ìˆë‹¤. ì´ëŸ¬í•œ êµ¬ë¬¸ë¡ ì ì¸ ì •ë³´ë¥¼ ì£¼ëŠ” ê²ƒì€ context, contentê°€ ì•„ë‹ˆë¼ absolute positionì´ë‹¤.</p>
<p>í•´ì„œ debertaì—ì„œëŠ” ì´ëŸ¬í•œ absolute position embeddingì„ softmax layer ì§ì „ì— ë„£ì–´ì¤€ë‹¤. ë°”ë¡œ ì´ ì§€ì ì´ ëª¨ë¸ì´ ì•ì—ì„œ ë§í•œ content, position embedding ì¡°í•©ì„ ê¸°ë°˜ìœ¼ë¡œ maskingì„ í•´ë…í•˜ê¸° ì§ì „ì´ë‹¤. ê·¸ë‹ˆê¹ ê±°ì˜ ë§ˆì§€ë§‰ì— ë„£ì–´ì£¼ëŠ” ê²ƒìœ¼ë¡œ â€˜ì°¸ì¡°â€™ì˜ ìˆ˜ì¤€ìœ¼ë¡œ í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.</p>
</section>
<section id="virtual-adversarial-training">
<h3>1.3 virtual adversarial training<a class="headerlink" href="#virtual-adversarial-training" title="Permalink to this heading">#</a></h3>
<p>ì´ê±´ ìì„¸í•˜ê²Œ ì„¤ëª…ì´ ì•ì— ì•ˆë‚˜ì˜¤ëŠ”ë° model generalizationì— ì¢‹ë‹¤ê³  í•œë‹¤.</p>
</section>
<section id="mlm">
<h3>1.4 MLM<a class="headerlink" href="#mlm" title="Permalink to this heading">#</a></h3>
<p>Masked Language Model</p>
<p>ê¸°ì¡´ì˜ large-scale transformer-based PLM(pretrained language model)ë“¤ì€ ë³´í†µ ë§ì€ ì–‘ì˜ í…ìŠ¤íŠ¸ë¥¼ ê°€ì§€ê³  ë¬¸ë§¥ì ì¸ ë‹¨ì–´ì˜ í‘œí˜„(contextual word representation)ì„ ë°°ìš°ê¸° ìœ„í•´ì„œ í•™ìŠµ ê³¼ì •ì„ ê±°ì³¤ë‹¤. ì´ í•™ìŠµ ê³¼ì •ì€ â€˜self-supervision objectiveâ€™ ê·¸ëŸ¬ë‹ˆê¹ ìê¸°ì§€ë„ í•™ìŠµì¸ë°, ë°©ë²•ë¡  ì ìœ¼ë¡œëŠ” MLMì„ ê°€ë¦¬í‚¨ë‹¤. ì´ì œ ìˆ˜ì‹ì ìœ¼ë¡œ ë“¤ì–´ê°€ë³´ì</p>
<div class="dropdown note admonition">
<p class="admonition-title">MLM</p>
<p>given a sequence <span class="math notranslate nohighlight">\(X = \{x_i\}\)</span><br />
corrupt it into <span class="math notranslate nohighlight">\(\tilde{X}\)</span> by 15% of its tokens at random<br />
model parameterized by <span class="math notranslate nohighlight">\(\theta\)</span><br />
train model to reconstruct <span class="math notranslate nohighlight">\(\tilde{X}\)</span><br />
by predicting the masked token <span class="math notranslate nohighlight">\(\tilde{x}\)</span> conditioned on <span class="math notranslate nohighlight">\(\tilde{X}\)</span></p>
<div class="math notranslate nohighlight">
\[
\max_{\theta}\log p_{\theta}(X|\tilde{X}) = \max_{\theta} \sum_{i\in C} \log p_{\theta}(x_i = x_i | \tilde{X})
\]</div>
<ul class="simple">
<li><p>C = index set of the masked tokens in sequence</p></li>
<li><p>10%ì˜ masked tokensëŠ” ë³€ì¹˜ì•Šì€ ëŒ€ë¡œ ë‘ê³ </p></li>
<li><p>ë‹¤ë¥¸ 10%ëŠ” ë¬´ì‘ìœ„ë¡œ ê³ ë¥¸ ê²ƒê³¼ ë°”ê¾¸ê³ </p></li>
<li><p>80%ëŠ” mask tokenìœ¼ë¡œ ë°”ê¾¸ë¼ê³  í•œë‹¤.</p></li>
</ul>
</div>
</section>
</section>
<section id="deberta-architecture">
<h2>DeBERTa architecture<a class="headerlink" href="#deberta-architecture" title="Permalink to this heading">#</a></h2>
<section id="id1">
<h3>3.1 Disentangled attention<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<p>a two-vector approach to content and position embedding</p>
<blockquote>
<div><p>$i<span class="math notranslate nohighlight">\( : position of token in a sequence\
\)</span>{H_i}<span class="math notranslate nohighlight">\( : content of token, hidden state, output from encoding\
\)</span>{P_{i|j}}$ : relative position with j-th token, from distance of tokens\</p>
</div></blockquote>
<p><strong>cross attention score token_i / token_j</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{matrix}
A_{i,j} &amp;=&amp; \{H_i,P_{i|j}\} \times \{H_i,P_{j|i}\}^{\intercal}\\
        &amp;=&amp; H_iH_j^{\intercal} + H_iP_{j|i}^{\intercal} + P_{i|j}H_j^{\intercal}+P_{i|j}P_{j|i}^{\intercal}\\
\end{matrix}
\end{split}\]</div>
<p>í•˜ë‚˜ì˜ ë‹¨ì–´ ìŒì— ëŒ€í•´ì„œ attention weightë¥¼ ê³„ì‚°í•˜ë ¤ë©´ 4 attention scoreë¥¼ êµ¬í•´ì•¼ í•œë‹¤. ì´ ê³¼ì •ì—ì„œ ì‚¬ìš©ë˜ëŠ” ê²ƒì´ <code class="docutils literal notranslate"><span class="pre">disentangled</span> <span class="pre">matrices</span></code>ì´ê³  ì´ëŠ” content position ë‘ ê°œì— ëŒ€í•´ì„œ <code class="docutils literal notranslate"><span class="pre">content-content</span></code>,<code class="docutils literal notranslate"><span class="pre">content-position</span></code>,<code class="docutils literal notranslate"><span class="pre">position-content</span></code>,<code class="docutils literal notranslate"><span class="pre">position-position</span></code>ë¡œ 4ê°œê°€ ëœë‹¤.</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(H_i H_j^{\intercal}\)</span>: ië²ˆì§¸ í† í°ì˜ ë‚´ìš© ì •ë³´ì™€ jë²ˆì§¸ í† í°ì˜ ë‚´ìš© ì •ë³´ ê°„ì˜ ìƒí˜¸ì‘ìš©ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì´ëŠ” í† í° ê°„ì˜ ë‚´ìš©ì ì¸ ìœ ì‚¬ì„±ì„ ì¸¡ì •í•©ë‹ˆë‹¤.<code class="docutils literal notranslate"><span class="pre">content-content</span></code></p></li>
<li><p><span class="math notranslate nohighlight">\(H_i P_{j|i}^{\intercal}\)</span>: ië²ˆì§¸ í† í°ì˜ ë‚´ìš© ì •ë³´ì™€ jë²ˆì§¸ í† í°ì— ëŒ€í•œ ië²ˆì§¸ í† í°ì˜ ìƒëŒ€ì  ìœ„ì¹˜ ì •ë³´ ê°„ì˜ ìƒí˜¸ì‘ìš©ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì´ëŠ” ië²ˆì§¸ í† í°ì˜ ë‚´ìš©ì´ jë²ˆì§¸ í† í°ì— ëŒ€í•œ ìœ„ì¹˜ì— ì–´ë–»ê²Œ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.<code class="docutils literal notranslate"><span class="pre">content-position</span></code></p></li>
<li><p><span class="math notranslate nohighlight">\(P_{i|j} H_j^{\intercal}\)</span>: ië²ˆì§¸ í† í°ì— ëŒ€í•œ jë²ˆì§¸ í† í°ì˜ ìƒëŒ€ì  ìœ„ì¹˜ ì •ë³´ì™€ jë²ˆì§¸ í† í°ì˜ ë‚´ìš© ì •ë³´ ê°„ì˜ ìƒí˜¸ì‘ìš©ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì´ëŠ” jë²ˆì§¸ í† í°ì˜ ë‚´ìš©ì´ ië²ˆì§¸ í† í°ì— ëŒ€í•œ ìœ„ì¹˜ì— ì–´ë–»ê²Œ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.<code class="docutils literal notranslate"><span class="pre">position-content</span></code></p></li>
<li><p><span class="math notranslate nohighlight">\(P_{i|j} P_{j|i}^{\intercal}\)</span>: ië²ˆì§¸ í† í°ì— ëŒ€í•œ jë²ˆì§¸ í† í°ì˜ ìƒëŒ€ì  ìœ„ì¹˜ ì •ë³´ì™€ jë²ˆì§¸ í† í°ì— ëŒ€í•œ ië²ˆì§¸ í† í°ì˜ ìƒëŒ€ì  ìœ„ì¹˜ ì •ë³´ ê°„ì˜ ìƒí˜¸ì‘ìš©ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì´ëŠ” í† í° ê°„ì˜ ìƒëŒ€ì ì¸ ìœ„ì¹˜ì  ìœ ì‚¬ì„±ì„ ì¸¡ì •í•©ë‹ˆë‹¤.<code class="docutils literal notranslate"><span class="pre">position-position</span> <span class="pre">-</span> <span class="pre">relative</span> <span class="pre">position</span> <span class="pre">embeddingì—ì„œëŠ”</span> <span class="pre">ì œê±°ë˜ëŠ”</span> <span class="pre">ë¶€ë¶„</span></code></p></li>
</ol>
<p><strong>ì´ 4ê°œë¥¼ disentangled attentionsë¼ê³  í•˜ëŠ” ê²ƒì´ë‹¤</strong><br />
ì´ì „ì˜ relative position encodingì€ ê¸°ì¡´ì˜ attention weightì—ë‹¤ê°€ relative position biasë¥¼ ë”í•´ì£¼ëŠ” ì‹ìœ¼ë¡œ ìœ„ì—ì„œì˜ 1,2ë²ˆë§Œì„ ì‚¬ìš©í•˜ëŠ” ì‹ìœ¼ë¡œ ì‚¬ìš©ë˜ì—ˆë‹¤. í•˜ì§€ë§Œ debertaì—ì„œëŠ” ì´ë ‡ê²Œ ëª¨ë“  ê²ƒì„ ì‚¬ìš©í•˜ë˜ 4ë²ˆì€ ì•ˆì‚¬ìš©í•œë‹¤. ì´ë¯¸ 1,2,3ì—ì„œ ë‘ í† í° ê°„ì˜ ìƒëŒ€ì  ìœ„ì¹˜ ì •ë³´ë¥¼ ì¶©ë¶„íˆ captureí•œë‹¤ê³  íŒë‹¨í–ˆê¸° ë•Œë¬¸ì— ìˆ˜ì‹ìƒìœ¼ë¡œëŠ” outë˜ì—ˆë‹¤ê³  í•œë‹¤.</p>
</section>
<section id="standard-self-attention-operation">
<h3>3.1.1 standard self-attention operation<a class="headerlink" href="#standard-self-attention-operation" title="Permalink to this heading">#</a></h3>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(R^{N \times d}\)</span> : N(ë¬¸ì¥ì˜ ê¸¸ì´, í† í°ì˜ ìˆ˜)ê°œì˜ í–‰ * d(í† í°ì˜ hidden ë²¡í„°ì˜ ì°¨ì›)ê°œì˜ ì—´ - ì°¨ì›ì˜ ì‹¤ìˆ˜Â® í–‰ë ¬<br />
<span class="math notranslate nohighlight">\(H \in R^{N \times d}\)</span> : input hidden vectors<br />
<span class="math notranslate nohighlight">\(H_o \in R^{N \times d}\)</span> : output of self-attention<br />
<span class="math notranslate nohighlight">\(W_q,W_k,W_v \in R^{d \times d}\)</span> : projection matrices<br />
<span class="math notranslate nohighlight">\(A \in R^{N \times N}\)</span> : attention matrix</p>
</div></blockquote>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{matrix}
Q &amp;=&amp; HW_q,\\
K &amp;=&amp; HW_K,\\
V &amp;=&amp; HW_v,\\
A &amp;=&amp; \frac{QK^T}{\sqrt{d}}\\
H_o &amp;=&amp; \text{softmax}(A)V\\
\end{matrix}
\end{split}\]</div>
<p>ì—¬ê¸°ì„œ Q, K, VëŠ” ê°ê° query, key, valueë¥¼ ë‚˜íƒ€ë‚´ë©°, d_këŠ” keyì˜ ì°¨ì›ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì´ ë•Œ ê° í† í°ì˜ ìœ„ì¹˜ ì •ë³´ëŠ” absolute positional encoding ë°©ì‹ì„ í†µí•´ ê° í† í°ì˜ hidden stateì— ì¶”ê°€ë˜ê³ , ì´ ì •ë³´ê°€ í¬í•¨ëœ hidden stateê°€ query, key, valueë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.</p>
</section>
<section id="relative-distance">
<h3>3.1.2 relative distance<a class="headerlink" href="#relative-distance" title="Permalink to this heading">#</a></h3>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(k\)</span> : maximum relative distance<br />
<span class="math notranslate nohighlight">\(\delta(i,j)\in[0,2k)\)</span> : the relative distance from token i to token j</p>
</div></blockquote>
<div class="math notranslate nohighlight">
\[\begin{split}
\delta(i,j) =
\begin{cases}
0 &amp; \text{for} &amp; i-j\leqslant-k\\
2k-1 &amp; \text{for} &amp; i-j\geqslant k\\
i-j+k &amp;\text{others.} &amp;\\
\end{cases}
\end{split}\]</div>
<p>ì—¬ê¸°ì„œ [ : í¬í•¨, ) : ë¯¸í¬í•¨ì„ ì˜ë¯¸í•œë‹¤. ìƒëŒ€ì  ê±°ë¦¬ê°’ì´ 0&lt;=x&lt;2k ì˜ ëœ»ì´ë¼ê³  í•œë‹¤. 0~2k-1ì‚¬ì´ì˜ ì •ìˆ˜ ê°’</p>
</section>
<section id="disentangled-self-attention">
<h3>3.1.3 disentangled self-attention<a class="headerlink" href="#disentangled-self-attention" title="Permalink to this heading">#</a></h3>
<p>with relative position bias as equation 4</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(Q_c,K_c,V_c\)</span> : projected content vectors from projection matrices<br />
<span class="math notranslate nohighlight">\(W_{q,c},W_{k,c},W_{v,c} \in R^{d\times d}\)</span> : projection matrices<br />
<span class="math notranslate nohighlight">\(P \in R^{2k\times d}\)</span> : relative position embedding vectors shared with all layers<br />
<span class="math notranslate nohighlight">\(Q_r, K_r\)</span> : projected relative position vectors<br />
<span class="math notranslate nohighlight">\(W_{q,r},W_{k,r} \in R^{d\times d}\)</span> : projection matrices</p>
</div></blockquote>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{matrix}
Q_c &amp;=&amp; HW_{q,c},\\
K_c &amp;=&amp; HW_{k,c},\\
V_c &amp;=&amp; HW_{v,c},\\
Q_r &amp;=&amp; PW_{q,r},\\
K_r &amp;=&amp; PW_{k,r}\\
\end{matrix}
\end{split}\]</div>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(\tilde{A}_{i,j}\)</span> : <span class="math notranslate nohighlight">\(\tilde{A}\)</span>ì˜ ìš”ì†Œ. í† í° iì—ì„œ token j ë¡œì˜ ì–´í…ì…˜ ìŠ¤ì½”ì–´<br />
<span class="math notranslate nohighlight">\(Q_i^c\)</span> : i-th row of <span class="math notranslate nohighlight">\(Q_c\)</span><br />
<span class="math notranslate nohighlight">\(Q^r_{\delta(j,i)}\)</span> : <span class="math notranslate nohighlight">\(Q_r\)</span> with regarding to ìƒëŒ€ì  ê±°ë¦¬ <span class="math notranslate nohighlight">\(\delta(j,i)\)</span></p>
</div></blockquote>
<div class="math notranslate nohighlight">
\[
\tilde{A}_{i,j} = Q_i^cK_j^{c\intercal} + Q_i^c{K_{\delta(i,j)}^r}^\intercal + K_j^c{Q_{\delta(j,i)^r}}^\intercal
\]</div>
</section>
<section id="projection-matrix">
<h3>projection matrix<a class="headerlink" href="#projection-matrix" title="Permalink to this heading">#</a></h3>
<p>í•œ ë²¡í„°ë‚˜ ê³µê°„ì„ ë‹¤ë¥¸ ê³µê°„ìœ¼ë¡œ mappingí•˜ëŠ” ê²ƒì„ ë„ìš°ëŠ” matrix</p>
<script
   type="text/javascript"
   src="https://utteranc.es/client.js"
   async="async"
   repo="surdarla/surdarla.github.io"
   issue-term="pathname"
   theme="github-light"
   label="ğŸ’¬ comment"
   crossorigin="anonymous"
/></section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./scripts\paper_review"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="DebertaV3.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Deberta V3</p>
      </div>
    </a>
    <a class="right-next"
       href="../algorithm/problem-solving.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">problem-solving</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">ë¬¸ì œ ì„¤ì • Introduction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parallelization">1.1 Parallelization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#disentangled-attention">1.2 Disentangled attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#enhanced-mask-decoder">1.3 Enhanced mask decoder</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#virtual-adversarial-training">1.3 virtual adversarial training</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mlm">1.4 MLM</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deberta-architecture">DeBERTa architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">3.1 Disentangled attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#standard-self-attention-operation">3.1.1 standard self-attention operation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#relative-distance">3.1.2 relative distance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#disentangled-self-attention">3.1.3 disentangled self-attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#projection-matrix">projection matrix</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Surdarla
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      Â© Copyright 2023, Surdarla.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  based on jupyter-book, last updated: 2023-11-24 16:51:00
</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>