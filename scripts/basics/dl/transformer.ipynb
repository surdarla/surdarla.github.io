{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## attention\n",
    "\n",
    "> a vector of importance weights : in order to predict or infer one element, such as picel in an image or a word in a sentence.\n",
    "\n",
    "how strongly it is correlated with(or attends to) other elements and take the sum of their values weighted by the attention vector as the approximation of the target\n",
    "\n",
    "이미지와 텍스트 상에서 하나의 원소는 pixel, word일 것이다. 이에 대해서 attention은 다른 것들과 연관성이 얼마나 강한지를 표현하는 역할을 해준다. 그런데 이에 대해서는 그동안 seq2seq가 동일한 역할을 했었지만 long-term problem 때문에 멀리 있는 요소에 대한 기억력 감소로 attention을 사용하게 된다. seq2seq는 2014년에 attention은 2015년에 transformer는 2016년에 각각 나왔다.\n",
    "\n",
    "\n",
    "> the secret sauce invented by attention is to create shortcuts between the context vector and the entire source input. The weights of these shortcut connections are customizable for each output element.\n",
    "\n",
    "기계 번역 작업의 목표는 A 언어 문장을 B 언어로 자동으로 번역하는 것이다. 초기 기계 번역 모델에서는 전체 소스 문장을 하나의 고정된 컨텍스트 벡터로 요약하려고 시도했다. 그러니 이 방식은 긴 문장 처리와 번역 품질에 한계가 존재했다. 그래서 attention이 등장한다.\n",
    "어텐션 Mechanism은 인코더의 마지막 숨겨진 상태 하나로부터 단일 컨텍스트 벡터를 만드는(이전까지의 seq2seq방식) 대신, 소스 입력 전체와 컨텍스트 벡터 사이에 지름길 연결을 만드는 것이다.\n",
    "\n",
    "어텐션은 이러한 shortcut connection weight를 각 출력 요소(번역의 각 위치에 대한 토큰)에 대해서 커스터마이징 할 수 있도록 한다. 이는 모델이 출력 생성 중에 어떤 부분에 더 집중해야하는지를 학습할 수 있게 한다.\n",
    "\n",
    "컨텍스트 벡터는 전체 소스 입력 시퀀스에 대해서 접근할 수 있다. 따라서 긴 소스 문장을 처리하는데 있어서 문제가 없으며, 모델은 소스와 대상 간의 정렬을 학습하고 제어한다. 여기서 정렬을 학습한다는 것은 단어간 상응 관계를 학습한다는 것을 의미한다. 언어마다 문법에 따라 어순과 목적어가 와야할 위치가 다를 것인데, 언어는 달라도 의미가 같은 문장에서는 요소들이 위치가 다를 수 있다. 그에 대한 학습을 한다는 것이다. 이러한 것에 대한 학습을 하는 과정이 곧 언어를 기계가 학습하는 것으로 볼 수 있다.\n",
    "\n",
    "컨텍스트 벡터는 여러 정보 소스를 결합하여 출력을 생성하게 된다. 컨텍스트 벡터는 출력 요소마다 업데이트되며, 각 요소에 대한 번역에 대한 정보를 제공한다.\n",
    "1. Encoder hidden state : 기존 정보\n",
    "2. Decoder hidden state : 이전 출력 정보\n",
    "3. 소스와 대상 간의 정렬 정보\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shortcut\n",
    "\n",
    "shortcut connection은 듣는 순간 떠오르는 논문이 있다. 바로 resnet이다. resnet에서는 deep nn에서 발생하는 그래디언트 소실 문제를 해결하기 위해 나온 아키텍쳐로 굉장히 센세이셔널 했던 구조이다. 지금까지도 많은 아키텍쳐들의 백본으로 사용되기도 할만큼 그 아키텍쳐가 가지는 구조적 강점이 크다. 그 중에서도 중요했던 것이 이 숏컷이다.\n",
    "\n",
    "Resnet에서의 숏컷은 `Residual connection`이라고도 불린다. 이는 네트워크 레이어를 건너뛰고 입력과 출력을 직접 연결함으로써 그래디언트가 원활하게 흐를 수 있도록 도왔다.\n",
    "\n",
    "Attention에서의 숏컷은 `인코더와 디코더 사이의 연결`을 말한다. 각 출력 요소가 소스 입력의 모든 요소에 대한 가중합을 계산하고 이를 사용하여 출력을 생성하는 메커니즘을 설명한다. 이는 모델이 입력과 출력 간에 관계를 학습하는데 사용한다는 점에서 다르다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 참조\n",
    "\n",
    "- [lil'Log - Attention?Attention!](https://lilianweng.github.io/posts/2018-06-24-attention/#born-for-translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{raw} html\n",
    "<script\n",
    "   type=\"text/javascript\"\n",
    "   src=\"https://utteranc.es/client.js\"\n",
    "   async=\"async\"\n",
    "   repo=\"surdarla/surdarla.github.io\"\n",
    "   issue-term=\"pathname\"\n",
    "   theme=\"github-light\"\n",
    "   label=\"💬 comment\"\n",
    "   crossorigin=\"anonymous\"\n",
    "/>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
