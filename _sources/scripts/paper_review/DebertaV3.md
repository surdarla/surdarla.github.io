
# Deberta V3

DEBERTAV3: IMPROVING DEBERTA USING ELECTRA-STYLE PRE-TRAINING WITH GRADIENT-DISENTANGLED EMBEDDING SHARING

Published as a conference paper at ICLR 2023 {bdg-link-primary}`Paper PDF<http://arxiv.org/abs/2111.09543>`

```{admonition} Abstract
:class: dropdown, note

This paper presents a new pre-trained language model, `DeBERTaV3`, which improves the original DeBERTa model by replacing masked language modeling(MLM) with `replaced token detection (RTD)`, a more sample-efï¬cient pre-training task. Our analysis shows that vanilla embedding sharing in ELECTRA hurts training efï¬ciency and model performance, because the training losses of the discriminator and the generator pull token embeddings in different directions, creating the â€œtug-of-warâ€ dynamics. We thus propose a new gradient-disentangled embedding sharing method that avoids the tug-of-war dynamics, improving both training efï¬ciency and the quality of the pre-trained model. We have pre-trained DeBERTaV3 using the same settings as DeBERTa to demonstrate its exceptional performance on a wide range of downstream natural language understanding (NLU) tasks. Taking the GLUE benchmark with eight tasks as an example, the DeBERTaV3 Large model achieves a 91.37% average score, which is 1.37% higher than DeBERTa and 1.91% higher than ELECTRA, setting a new state-of-the-art (SOTA) among the models with a similar structure. Furthermore, we have pre-trained a multilingual model mDeBERTaV3 and observed a larger improvement over strong baselines compared to English models. For example, the mDeBERTaV3 Base achieves a 79.8% zero-shot cross-lingual accuracy on XNLI and a 3.6% improvement over XLM-R Base, creating a new SOTA on this benchmark. Our models and code are publicly available at https://github.com/microsoft/DeBERTa.
```

## ë¬¸ì œ ì„¤ì •

PLMs(Pre-trained Language Models)ë¥¼ scaling upì„ í•´ì„œ íŒŒë¼ë¯¸í„°ë¥¼ ìˆ˜ì‹­ì–µ ìˆ˜ë°±ë§Œ ë‹¨ìœ„ë¡œ ëŠ˜ë¦¬ëŠ” ê²ƒì´ í™•ì‹¤í•œ ì„±ëŠ¥í–¥ìƒì´ ìˆì—ˆê³  ì§€ê¸ˆê¹Œì§€ì˜ ì£¼ë„ì ì¸ ë°©ë²•ì´ì—ˆì§€ë§Œ, ë” ì£¼ìš”í•œ ê²ƒì€ parameterë¥¼ ì¤„ì´ê³  computation costë¥¼ ì¤„ì´ëŠ” ê²ƒì´ë¼ê³  ë§í•œë‹¤.

Improving Efficency

1. incorporating disentangled attention(imporoved relative-position encoding mechanism)
   - DeBERTAëŠ” 1.5Bê¹Œì§€ scaling upì„ í•¨ìœ¼ë¡œì¨ SuperGLUEì—ì„œ ì²˜ìŒìœ¼ë¡œ ì‚¬ëŒ performanceë¥¼ ë„˜ì–´ì„°ë‹¤.
1. **Replaced Token Detection(RTD)** vs Masked language modeling(MLM)
   - proposed by ELECTRA(2020)
   - transformer encoderë¥¼ ì˜¤ì—¼ëœ tokenë¥¼ ì˜ˆì¸¡í•˜ëŠ”ë° ì‚¬ìš©í•˜ë˜ BERT(MLM)ì™€ëŠ” ë‹¤ë¥´ê²Œ
   - RTDëŠ” generator, discriminator ë¥¼ ì‚¬ìš©í•œë‹¤. generatorëŠ” í—¤ê¹”ë¦¬ëŠ” ì˜¤ì—¼ì„ ë§Œë“¤ì–´ë‚´ê³ , discriminatorëŠ” generatorê°€ ë§Œë“  ì˜¤ì—¼ëœ í† í°ì„ original inputsê³¼ êµ¬ë¶„ì„ í•´ë‚´ë ¤í•œë‹¤. ë§ˆì¹˜ GAN(Generative Adversarial Networks)ë‘ ìƒë‹¹íˆ ë¹„ìŠ·í•œ ë©´ì´ ìˆë‹¤. ì°¨ì´ì ì— ëŒ€í•´ì„œ ë¶„ëª…í•˜ê²Œ í•˜ì.

ì—¬ê¸°ì„œ ì´ì „ì˜ DeBERTaì—ì„œ V3ë¡œ ë‚˜ì•„ê°€ë©´ì„œ ë°”ë€ ì ìœ¼ë¡œ ë‘ ê°€ì§€ë¡œ ê¼½ëŠ”ë‹¤.
í•˜ë‚˜ëŠ” ìœ„ì—ì„œ ë§í•œ BERTì˜ MLMì„ ELECTRA ìŠ¤íƒ€ì¼ì˜ RTD(where the model is trained as a discriminator to predict whethre a token in the corrupt input is either original or replaced by a generator)ë¡œ ë°”ê¾¸ëŠ” ê²ƒì´ê³ , ë‹¤ë¥¸ í•˜ë‚˜ëŠ” new embedding sharing methodì´ë‹¤. ELECTRAì—ì„œ generator discriminatorëŠ” ê°™ì€ token embeddingì„ ê³µìœ í•œë‹¤.

ê·¸ëŸ¬ë‚˜ ì—°êµ¬ìë“¤ì€ ë³¸ì¸ë“¤ì˜ ì—°êµ¬ì— ë”°ë¥´ë©´ ì´ê²ƒì€ í•™ìŠµ íš¨ìœ¨ì„± ë©´ì´ë‚˜ ëª¨ë¸ì˜ ì„±ëŠ¥ë©´ì—ì„œ ë¶€ì •ì ì¸ ì˜í–¥ì„ ì¤€ë‹¤ê³  ë§í•œë‹¤. `training losses of the discriminator and the generator pull token embeddings into opposite directions` ì¦‰ ìƒì„±ê¸°ì™€ ë¶„ë¥˜ê¸°ì˜ í•™ìŠµì˜ ë°©í–¥ì„±ì´ ë°˜ëŒ€ì´ê¸° ë•Œë¬¸ì— í•™ìŠµ lossê°€ ê°ˆíŒ¡ì§ˆíŒ¡í•  ìˆ˜ ë°–ì— ì—†ë‹¤ëŠ” ê²ƒì´ë‹¤. ê·¸ëŸ¼ ìƒê°í•´ë³¼ ìˆ˜ ìˆëŠ” ê²ƒì´ ë’¤ì— ë‘ê°œì˜ lossë¥¼ ë§Œë“¤ì–´ì„œ í•™ìŠµ ë°©í–¥ì„±ì„ ë°˜ëŒ€ë¡œ ê°ˆ ìˆë„ë¡ í–ˆì„ê¹Œ í•˜ëŠ” ì ì„ ìƒê°í•´ ë³¼ ìˆ˜ ìˆê² ë‹¤. token embeddingì„ ë‹¬ë¦¬ ì¤€ë‹¤ëŠ” ê²ƒì´ ì–´ë–¤ ì˜ë¯¸ì¸ì§€ ë’¤ì—ì„œ í™•ì‹¤í•˜ê²Œ ë‚˜ì™€ì•¼ í•  ê²ƒì´ë‹¤. MLMì€ generatorë¥¼ token ì¤‘ì—ì„œ ì„œë¡œ ê´€ë ¨ì´ ìˆì–´ë³´ì´ëŠ” ê°€ê¹Œìš´ ê²ƒë“¤ì„ ì„œë¡œ ì¡ì•„ë‹¹ê¸°ë©´ì„œ í•™-ìŠµì„ ì§„í–‰í•˜ê²Œ ëœë‹¤. í•˜ì§€ë§Œ ë°˜ë©´ì— RTDì˜ discriminatorëŠ” ì˜ë¯¸ì ìœ¼ë¡œ ê°€ê¹Œìš´ tokenì˜ ì‚¬ì´ë¥¼ ìµœëŒ€í•œ ë©€ë¦¬í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì´ì§„ë¶„ë¥˜ ìµœì í™”(ë§ë‹¤ ì•„ë‹ˆë‹¤)ë¥¼ í•˜ê³  pull their embeddingì„ í•˜ê²Œ ë¨ìœ¼ë¡œì¨ ë”ìš± êµ¬ë¶„ì„ ì˜ í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê²ƒì´ë‹¤. ì´ëŸ¬í•œ 'ì¤„ë‹¤ë¦¬ê¸° tug-of-war'ì™€ ê°™ì€ ì—­í•™ì´ í•™ìŠµì„ ë§ì¹˜ê³ , ëª¨ë¸ ì„±ëŠ¥ì„ ë–¨ì–´íŠ¸ë¦¬ëŠ” ê²ƒì´ë¼ê³  ë§í•œë‹¤. ê·¸ë ‡ë‹¤ê³  ë¬´ì¡°ê±´ì ìœ¼ë¡œ seperated embeddingì„ í•  ìˆ˜ëŠ” ì—†ëŠ” ê²ƒì´ generatorì˜ embeddingì„ discriminatorì˜ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ taskí•™ìŠµì— í¬í•¨ì‹œí‚¤ëŠ” ê²ƒì´ ë„ì›€ì´ ëœë‹¤ê³  ë§í•˜ëŠ” ë…¼ë¬¸ë„ ìˆì—ˆê¸° ë•Œë¬¸ì´ë‹¤.

ê·¸ë˜ì„œ ê·¸ë“¤ì´ ì œì•ˆí•˜ëŠ” ê²ƒì€ `new gradient-disentangled embedding sharing(GDES) method`ì´ë‹¤. the generator shares its embeddings with the discriminator but stops the gradients from the discriminator to the generator embeddings. embedding sharingì˜ ì¥ì ë§Œì„ ì·¨í•˜ë˜, ì¤„ë‹¤ë¦¬ê¸° ì—­í•™ì€ í”¼í•  ìˆ˜ ìˆë„ë¡ gradientì˜ íë¦„ì´ discriminatorì—ì„œ generatorë¡œ íë¥´ì§€ëŠ” ì•Šë„ë¡ í•˜ëŠ” ë°©ì‹ì¸ ê²ƒì´ë‹¤.

### Model Table

| Model                                                                               | Vocabulary(K) | Backbone Parameters(M) | Hidden Size | Layers | Note                                                       |
|-------------------------------------------------------------------------------------|---------------|------------------------|-------------|--------|------------------------------------------------------------|
| **[V2-XXLarge](https://huggingface.co/microsoft/deberta-v2-xxlarge)<sup>1</sup>**   | 128           | 1320                   | 1536        | 48     | 128K new SPM vocab                                         |
| [V2-XLarge](https://huggingface.co/microsoft/deberta-v2-xlarge)                     | 128           | 710                    | 1536        | 24     | 128K new SPM vocab                                         |
| [XLarge](https://huggingface.co/microsoft/deberta-xlarge)                           | 50            | 700                    | 1024        | 48     | Same vocab as RoBERTa                                      |
| [Large](https://huggingface.co/microsoft/deberta-large)                             | 50            | 350                    | 1024        | 24     | Same vocab as RoBERTa                                      |
| [Base](https://huggingface.co/microsoft/deberta-base)                               | 50            | 100                    | 768         | 12     | Same vocab as RoBERTa                                      |
| [DeBERTa-V3-Large](https://huggingface.co/microsoft/deberta-v3-large)<sup>2</sup>   | 128           | 304                    | 1024        | 24     | 128K new SPM vocab                                         |
| [DeBERTa-V3-Base](https://huggingface.co/microsoft/deberta-v3-base)<sup>2</sup>     | 128           | 86                     | 768         | 12     | 128K new SPM vocab                                         |
| [DeBERTa-V3-Small](https://huggingface.co/microsoft/deberta-v3-small)<sup>2</sup>   | 128           | 44                     | 768         | 6      | 128K new SPM vocab                                         |
| [DeBERTa-V3-XSmall](https://huggingface.co/microsoft/deberta-v3-xsmall)<sup>2</sup> | 128           | 22                     | 384         | 12     | 128K new SPM vocab                                         |
| [mDeBERTa-V3-Base](https://huggingface.co/microsoft/mdeberta-v3-base)<sup>2</sup>   | 250           | 86                     | 768         | 12     | 250K new SPM vocab, multi-lingual model with 102 languages |

> ì°¸ì¡°
>
> 1. This is the model(89.9) that surpassed T5 11B(89.3) and human performance(89.8) on SuperGLUE for the first time. 128K new SPM vocab.
> 2. These V3 DeBERTa models are deberta models pre-trained with ELECTRA-style objective plus gradient-disentangled embedding sharing which significantly improves the model efficiency.

## Background

### 1. Transformer

Transformer ê¸°ë°˜ ì–¸ì–´ëª¨ë¸ë“¤ì€ $L$ê°œì˜ transformer blockì´ ìŒ“ì—¬ì§„ í˜•íƒœë¡œ êµ¬ì„±ëœë‹¤. ê° ë¸”ë½ë“¤ì€ multi-head self-attention layerë“¤ì„ í¬í•¨í•˜ê³  ê·¸ ë’¤ë¡œì€ fully-connected positional feed-forward networkê°€ ë’¤ë”°ë¥¸ë‹¤. ê¸°ì¡´ì˜ self-attention ë©”ì»¤ë‹ˆì¦˜ì€ ë‹¨ì–´ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ encodeí•˜ëŠ”ë°ëŠ” ì í•©í•˜ì§€ ì•Šì•˜ë‹¤. ê·¸ë˜ì„œ ê¸°ì¡´ì˜ ì ‘ê·¼ë²•ë“¤ì€ positional biasë¥¼ ê° input word embeddingì— ë”í•¨ìœ¼ë¡œì¨, contentì™€ positionì— ë”°ë¼ì„œ ê°’ì´ ë‹¬ë¼ì§€ëŠ” vectorë¡œ í‘œí˜„í•˜ë ¤ê³  í•˜ì˜€ë‹¤. ì´ positional biasëŠ” absolute position embedding, relative position embedding ë“±ì´ ìˆì—ˆë‹¤. ìƒëŒ€ì  ìœ„ì¹˜ ì„ë² ë”©ì´ ì¢‹ì€ ê²°ê³¼ë“¤ì„ ìµœê·¼ê¹Œì§€ëŠ” ë³´ì—¬ì£¼ê³  ìˆëŠ” ì¶”ì„¸ë¼ê³  í•œë‹¤.

### 2. DeBERTa

BERTë¡œë¶€í„° ë‘ ê°€ì§€ ê°œì„ ì ì„ ë³´ì—¬ì¤€ë‹¤. ìš°ì„  DA(Disentengled Attention : ë¶„ë¦¬ëœ ì–´í…ì…˜), ê·¸ë¦¬ê³  enhanced mask decoderì´ë‹¤. ì´ì „ì˜ single vectorë¡œ í•˜ë‚˜ì˜ input wordì˜ ë‚´ìš©ê³¼ ìœ„ì§€ì •ë³´ë¥¼ í‘œí˜„í•˜ë ¤ëŠ” ê²ƒê³¼ëŠ” ë‹¤ë¥¸ê²Œ, DAëŠ” ë‘ ê°œì˜ seperate vectorë¥¼ ì‚¬ìš©í•œë‹¤. `one for the content and one for the position`. ê·¸ëŸ¬ë©´ì„œ DA ë©”ì»¤ë‹ˆì¦˜ì˜ ë‹¨ì–´ë“¤ ì‚¬ì´ì˜ attention weightëŠ” disentangled matricesì— ì˜í•´ì„œ ê³„ì‚°ë˜ê³  ì´ëŠ” ê°ê° ë‚´ìš©ê³¼ ìƒëŒ€ì  ìœ„ì¹˜ ë‘ ê°œê°€ ê°ê°ì˜ í–‰ë ¬ë¡œ ë‹¤ë¥´ê²Œ ê³„ì‚°ëœë‹¤.

ê·¸ë¦¬ê³  MLMì— ëŒ€í•´ì„œëŠ” BERTì™€ ë™ì¼í•˜ê²Œ ì‚¬ìš©ëœë‹¤. DAê°€ ì´ë¯¸ ë‚´ìš©ê³¼ ìƒëŒ€ì  ìœ„ì¹˜ì— ëŒ€í•œ ê³ ë¯¼ì´ ë“¤ì–´ê°€ëŠ” ìˆì§€ë§Œ, ì¤‘ìš”í•œ ê²ƒì€ absolute positionì— ëŒ€í•œ ê³ ë¯¼ì€ ì—†ë‹¤. absolute positionì€ ì˜ˆì¸¡ì—ì„œ ê½¤ë‚˜ ì£¼ìš”í•œ ìš”ì†Œì„ìœ¼ë¡œ ì´ëŸ¬í•œ ì ì„ ë³´ì™„í•˜ê¸° ìœ„í•´ì„œ DeBERTaì—ì„œëŠ” enhanced Mask Decoderë¥¼ MLMì„ ë³´ì™„í•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©í•œë‹¤. ì´ëŠ” MLM decoding layerì—ì„œ context wordì— absolute position informationì´ ì¶”ê°€ë¡œ ë“¤ì–´ê°€ëŠ” ë°©ì‹ì´ë‹¤.

### 3. ELECTRA

#### 2.3.1 Masked Language Model(MLM)

Large-scale Transformer-based PLMs ëŠ” ë³´í†µ ë§ì€ ì–‘ì˜ í…ìŠ¤íŠ¸ë¡œ ì‚¬ì „í•™ìŠµë˜ë©´ì„œ self-supervision objective(ìê¸°ì£¼ë„í•™ìŠµì˜ ëª©ì )ì¸ MLMì„ ì‹¤í˜„í•˜ê³  ì´ ë§ì¸ ì¦‰ìŠ¨ ë¬¸ë§¥ì„ ì´í•´í•˜ê²Œ ëœë‹¤ëŠ” ê²ƒì´ë‹¤.

$X = \{x_i\}$ëŠ” í•˜ë‚˜ì˜ sequenceì´ê³  $\tilde{X}$ëŠ” 15%ì˜ í† í°ì´ ì˜¤ì—¼(masking)ëœ ê²ƒì´ë‹¤. ëª©í‘œëŠ” ë‹¤ì‹œ reconstruct $X$ì´ë‹¤. ë°©ë²•ì€ language modelì„ predicting the masked tokens $\tilde{x} \text{ conditioned on } \tilde{X}$ í•˜ë©´ì„œ í›ˆë ¨ì‹œí‚¤ëŠ” ê²ƒì´ê³  parameterized by $\theta$ì´ë‹¤.

$$
\max_{\theta}\log p_{\theta}(X|\tilde{X}) = \max_{\theta}\sum_{i\in C} \log p_{\theta}(\tilde{x}_i|\tilde{X})
$$

C : index set of the masked tokens in the sequence\
BERTì—ì„œëŠ” 10%ì˜ masked tokensë¥¼ ë°”ê¾¼ ìƒíƒœë¡œ ìœ ì§€í•˜ê³ , ë‹¤ë¥¸ 10%ì˜ ë¬´ì‘ìœ„ ì„ íƒëœ í† í°ì„ ë°”ê¾¸ì—ˆê³ , ë‚˜ë¨¸ì§€ 80%ëŠ” ì•„ì˜ˆ $[MASK]$ tokenìœ¼ë¡œ ìœ ì§€í–ˆë‹¤.

#### 2.3.2 Replaced Token Detection(RTD)

BERTëŠ” í•˜ë‚˜ì˜ transformer encoderë¥¼ ì‚¬ìš©í–ˆê³ , MLMìœ¼ë¡œ í›ˆë ¨ë˜ì—ˆë‹¤. ì´ì™€ ë‹¤ë¥´ê²Œ ELECTRAëŠ” ë‘ ê°œì˜ transformer encodersë¥¼ ì‚¬ìš©í•˜ë©´ì„œ GANì²˜ëŸ¼ í›ˆë ¨í–ˆë‹¤. Generator encoderëŠ” MLMìœ¼ë¡œ í›ˆë ¨ë˜ì—ˆê³ , discriminator encoderëŠ” `token-level binary classifier`ë¡œ í›ˆë ¨ë˜ì—ˆë‹¤. generatorëŠ” input sequenceì—ì„œ ë§ˆìŠ¤í‚¹ëœ tokenì„ ëŒ€ì²´í•  ambiguousí•œ í† í°ì„ ìƒì„±í–ˆë‹¤. ê·¸ë¦¬ê³  ì´ë ‡ê²Œ ë§Œë“¤ì–´ì§„ sequenceëŠ” dicriminatorë¡œ ë“¤ì–´ê°€ì„œ í•´ë‹¹ í† í°ì´ original í† í°ì´ ë§ëŠ”ì§€ ì•„ë‹ˆë©´ generatorê°€ ë§Œë“  í† í°ì¸ì§€ë¥¼ ì´ì§„ ë¶„ë¥˜í•œë‹¤. ê·¸ë¦¬ê³  ì´ ì´ì§„ë¶„ë¥˜í•˜ëŠ” ê²ƒì´ RTDì´ë‹¤. ì—¬ê¸°ì„œ parameterized ë˜ëŠ” ë¶€ë¶„ì´ $\theta_{G}$ëŠ” generatorì˜ íŒŒë¼ë¯¸í„°ì´ê³ , $\theta_{D}$ëŠ” discriminatorì˜ íŒŒë¼ë¯¸í„°ì´ë‹¤. Loss function of the generatorëŠ” ì•„ë˜ì™€ ê°™ë‹¤.

$$
L_{MLM} = \mathbb{E} \Big(-\sum_{i \in C}\log p_{\theta_G} \big(\tilde{x}_{i,G} = x_i|\tilde{X}_G \big) \Big)
$$

- $p_{\theta_G} \big(\tilde{x}_{i,G} = x_i|\tilde{X}_G \big) \Big)$ : ì´ ë¶€ë¶„ì´ Gê°€ $\tilde{X}_G$ë¥¼ $x_i$ë¡œ ì¬êµ¬ì„±í•  í™•ë¥ ì´ë‹¤.
- íŠ¹íˆë‚˜ maskingëœ token($\tilde{X}_G$)ì€ ì›ë³¸ì—ì„œ randomly maskingí•œ 15%ì´ë‹¤.

```{admonition} Logì˜ ì„±ì§ˆ
:class: dropdown, note

- logì•ì— ë¶™ì€ - ëŠ” ì¬êµ¬ì„±í•  í™•ë¥ ì´ ë†’ì•„ì§ˆìˆ˜ë¡ generatorê°€ ì¼ì„ ì˜í•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì—, ìœ„ì˜ ì¬êµ¬ì„±í•  í™•ë¥ ì´ ë†’ì•„ì§ˆìˆ˜ë¡ lossê°’ì€ ë‚®ì•„ì§€ëŠ” êµ¬ì„±ì´ ë˜ì–´ì•¼ ëª¨ë¸ì´ ëª©í‘œí•¨ìˆ˜ì— ì˜ê±°í•œ í•™ìŠµì„ ì§„í–‰í•˜ê¸°ì— ë¶™ì€ ê²ƒì´ë‹¤.
- logëŠ” 0~1ì‚¬ì´ì˜ í™•ë¥ ê°’$\mathbb{E}$ì„ 0~$\infty$ ê°’ìœ¼ë¡œ ë³€í™˜í•œë‹¤. ì´ë¥¼ í†µí•´ì„œ underflow(0 * 0 * ... -> 0ì— ì ì ë” ê°€ê¹Œì›Œì§€ëŠ”)ë¥¼ ë°©ì§€í•˜ë©°, ê³±ì…ˆì„ ë§ì…ˆìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì„±ì§ˆì„ ì´ìš©í•œë‹¤.
```

discriminatorì— ë“¤ì–´ê°€ëŠ” input sequencesëŠ” generatorì˜ output probabilityì— ë”°ë¼ì„œ new tokensì´ masked tokens ìë¦¬ë¥¼ ì±„ìš´ì±„ë¡œ ë“¤ì–´ì˜¨ë‹¤. ê·¸ë˜ì„œ iê°€ Cì•ˆì— ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ xì´ê³ (replaced tokenì´ ì•„ë‹ˆë¼ ì•„ì˜ˆ ì›ë³¸ì´ê¸° ë•Œë¬¸), indexê°€ Cì— ìˆëŠ”ê±°ë§Œ ë¹„êµë¥¼í•œë‹¤.

$$
\tilde{x}_{i,D} =
\begin{cases}
\tilde{x}_i \sim p_{\theta_G} \big(&\tilde{x}_{i,G} = x_i|\tilde{X}_G\big), & i\in C    \\
& x_i, &i\notin C
\end{cases}
$$

- $\sim$ : sim ê¸°í˜¸ëŠ” 'distributed as' ë˜ëŠ” 'has a distribution of'ë¡œ í•´ì„ë˜ë©°, 'íŠ¹ì • í™•ë¥  ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ëŠ” ì˜ë¯¸', 'ë”°ë¥¸ë‹¤, ë¶„í¬í•œë‹¤'ì…ë‹ˆë‹¤. ì—¬ê¸°ì„œ sim ê¸°í˜¸ëŠ” ì£¼ì–´ì§„ í™•ë¥  ë¶„í¬ $p_{\theta_G}$ì— ë”°ë¼ ë³€ìˆ˜ $\tilde{x}_i$ê°€ ë¶„í¬í•œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸.
- ë”°ë¼ì„œ, $\tilde{x}_i \sim p{\theta_G} (\tilde{x}_{i,G} = x_i|\tilde{X}_G)$ëŠ” ë³€ìˆ˜ $\tilde{x}_i$ê°€ ì¡°ê±´ë¶€ í™•ë¥  ë¶„í¬ $p{\theta_G} (\tilde{x}_{i,G} = x_i|\tilde{X}_G)$ë¥¼ ë”°ë¥¸ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ ë¶„í¬ëŠ” $p_{\theta_G}$ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§€ë©°, $\tilde{X}_G$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ $\tilde{x}_{i,G} = x_i$ì¸ ì¡°ê±´ë¶€ ë¶„í¬ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ìœ„ ìˆ˜ì‹ì˜ ì „ì²´ì ì¸ ì˜ë¯¸ëŠ”, ì¸ë±ìŠ¤ iê°€ ì§‘í•© Cì— í¬í•¨ë˜ì–´ ìˆìœ¼ë©´, $\tilde{x}_i$ëŠ” ì¡°ê±´ë¶€ ë¶„í¬ $p_{\theta_G} (\tilde{x}_{i,G} = x_i|\tilde{X}_G)$ë¥¼ ë”°ë¥´ê³ , ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ $\tilde{x}_i$ëŠ” $x_i$ì™€ ë™ì¼í•˜ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

$$
L_{RTD} = \mathbb{E}\Big( -\sum_{i}\log p_{\theta_D} \big(\mathbb{I}(\tilde{x_{i,D}} = x_i)|\tilde{X}_D,i   \big)  \Big)
$$

- $\mathbb{I}$ : indicator functionì„ ë§í•œë‹¤. $(\tilde{x_{i,D}} = x_i)$ë¥¼ ì¶©ì¡±í•˜ë©´ 1ì„ ë°˜í™˜í•˜ê³ , ì•„ë‹ˆë©´($\tilde{X}_D,i$) 0ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§í•œë‹¤. ë§¤ìš° ì—„ê²©í•œ í•¨ìˆ˜ë¡œ ë³¼ ìˆ˜ ìˆìœ¼ë©° í›„ë³´ëŠ” 'sigmoid', 'tahn', 'ReLU', 'Leaky ReLU'ë“±ì´ ìˆë‹¤.
- ìœ„ discriminatorì˜ loss functionì˜ inputì€ $\tilde{X_D}$ ì´ë©° ì´ê²ƒì€ ìœ„ì˜ 3ë²ˆ equationì˜ resultì´ë‹¤. ì¦‰ discriminatorì—ì„œ í•œë²ˆ ê±¸ëŸ¬ì ¸ ë‚˜ì˜¨ ê²ƒì´ loss functionì— ë“¤ì–´ê°€ëŠ” ê²ƒì´ë‹¤.

ì´ì²´ì ì¸ ELECTRAì˜ loss functionì€ ì•„ë˜ì™€ ê°™ì´ ì •ë¦¬í•  ìˆ˜ ìˆë‹¤.
$$
L = L_{MLM} + \lambda L_{RTD}
$$

- $\lambda$ : discriminator loss functionì— ëŒ€í•œ weightë¥¼ ë‚˜íƒ€ëƒ„. ëª¨ë¸ í•™ìŠµì—ì„œ í•´ë‹¹ ì†ì‹¤ì˜ ì¤‘ìš”ì„±ì„ ì¡°ì ˆí•˜ëŠ”ë° ì‚¬ìš©ë¨. ì—¬ê¸°ì„œëŠ” 50ì„ìœ¼ë¡œ MLMì— ë¹„í•´ì„œ 50ë°°ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë” ê³±í•´ì¤€ë‹¤ëŠ” ì˜ë¯¸ì„ìœ¼ë¡œ RTDì— ì—„ì²­ ì¤‘ìš”ì„±ì„ ë†’ê²Œ ì³ì£¼ëŠ” ê²ƒì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.

## DeBERTaV3

DeBERTa + RTD training loss + new weight-sharing method

### 3.1 DeBERTa + RTD

ELECTRAì—ì„œ ê°€ì ¸ì˜¨ RTD, ê·¸ë¦¬ê³  DeBERTa disentangled attention mechanismì˜ í•©ì€ í”„ë¦¬íŠ¸ë ˆì´ë‹ ê³¼ì •ì—ì„œ ê°„ë‹¨í•˜ê³  íš¨ê³¼ì ì¸ ê²ƒìœ¼ë¡œ íŒë³„ë˜ì—ˆë‹¤. ì´ì „ DeBERTaì—ì„œ ì‚¬ìš©ë˜ì—ˆë˜ MLM objectiveë¥¼ RTD objectiveë¡œ ë°”ê¿ˆìœ¼ë¡œì¨ ë”ìš± disentangled attention mechainsmì„ ë”ìš± ê°•í™”í•˜ëŠ” ê²ƒì´ë‹¤.

training ë°ì´í„°ë¡œëŠ” Wikipedia, bookcorpusì˜ ë°ì´í„°ê°€ ì‚¬ìš©ë˜ì—ˆë‹¤. generatorëŠ” discriminatorì™€ ê°™ì€ widthë¥¼ ê°€ì§€ë˜ depthëŠ” ì ˆë°˜ë§Œ ê°€ì ¸ê°„ë‹¤. batch sizeëŠ” 2048ì´ë©° 125,000 stepì´ í›ˆë ¨ë˜ì—ˆë‹¤. learning_rate = 5e-4, warmup_steps = 10,000, ê·¸ë¦¬ê³  ìœ„ì—ì„œ ë§í–ˆë“¯ì´ RTD loss functionì— ê°€ì¤‘ì¹˜ë¥¼ 50ì„ ì¤Œìœ¼ë¡œì„œ optimization hyperparameterë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤.

ê²€ì¦ ë°ì´í„°ë¡œëŠ” MNLI, SQuAD v2.0ì„ ì‚¬ìš©í•˜ì˜€ê³ , ì´ ë°ì´í„°ë“¤ì— ëŒ€í•œ ì •ë¦¬ë„ í•„ìš”í•  ê²ƒì´ë‹¤. ê²°ê³¼ì ìœ¼ë¡œ DeBERTaë¥¼ ì••ë„í•˜ì§€ë§Œ ë”ìš±ë” improvedë  ìˆëŠ” í¬ì¸íŠ¸ë¥¼ ë§í•˜ëŠ” ì§€ì ì´ ìˆë‹¤. token Embedding Sharing(ES) used for RTD(ê¸°ì¡´ì— ì‚¬ìš©ë˜ì—ˆë˜)ë¥¼ new Gradient-Disentangled Embedding Sharing(GDES) methodë¡œ ë°”ê¿ˆìœ¼ë¡œì¨ ë”ìš± ë°œì „ë  ê°€ëŠ¥ì„±ì´ ìˆë‹¤ê³  ë§í•œë‹¤.

### 3.2 Token Embedding Sharing (in ELECTRA)

ELECTRAì—ì„œëŠ” generatorì™€ discriminatorê°€ token embeddingì„ ê³µìœ í•œë‹¤. ì´ê²ƒì´ `Embedding Sharing(ES)`ì´ë‹¤. ì´ ë°©ë²•ì€ generatorê°€ discriminatorì— inputìœ¼ë¡œ ë“¤ì–´ê°ˆ ì •ë³´ë¥¼ ì œê³µí•¨ìœ¼ë¡œì¨ í•™ìŠµì— í•„ìš”í•œ parameterë¥¼ ì¤„ì—¬ì£¼ëŠ” ì—­í• ì„ í•˜ê³  í•™ìŠµì„ ìš©ì´í•˜ê²Œ í•´ì¤€ë‹¤. ê·¸ëŸ¬ë‚˜ ì•ì—ì„œ ë§í–ˆë“¯ì´ ë‘ ê¸°ì œì˜ ëª©ì  ë°©í–¥ì„±ì´ ë°˜ëŒ€ì´ê¸° ë•Œë¬¸ì— ì„œë¡œë¥¼ ë°©í•´í•˜ê³ , í•™ìŠµ ìˆ˜ë ´ì„ ì €í•´í•  ê°€ëŠ¥ì„±ì´ í¬ë‹¤.

- $E$ : token embeddings
- $g_E$ : gradients = $\frac{\delta L_{MLM}}{\delta E} + \lambda\frac{\delta L_{RTD}}{\delta E}$

ìœ„ì˜ equationì€ token embeddings(E)ê°€ ë‘ ê°œì˜ ì¼ì—ì„œì˜ gradientë¥¼ í•œ ë²ˆì— ì¡°ì •í•˜ë©´ì„œ updateëœë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ìœ„ì—ì„œ ë§í–ˆë“¯ì´ ì´ê²ƒì€ ì¤„ë‹¤ë¦¬ê¸° ì´ë‹¤. ì•„ì£¼ ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ update ì†ë„ë¥¼ ì¡°ì ˆí•˜ë©´ì„œ(small learning_rate, gradient clipping) í•™ìŠµì„ ì§„í–‰í•˜ë©´ ê²°ë¡ ì ìœ¼ë¡œëŠ” ìˆ˜ë ´ì„ í•˜ê¸°ëŠ” í•œë‹¤ê³  ë§í•œë‹¤. í•˜ì§€ë§Œ ë‘ ê°œì˜ taskê°€ ì •ë°˜ëŒ€ì˜ ëª©ì ì„ ê°€ì§„ë‹¤ë©´ ì´ê²ƒì€ êµ‰ì¥íˆ ë¹„íš¨ìœ¨ì ì´ë©°, í•´ë‹¹ ìƒí™©(MLM,RTD)ì€ ì •í™•íˆ ê·¸ëŸ° ìƒí™©ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ë‘ ê°œì˜ taskê°€ token embeddingì„ ì—…ë°ì´íŠ¸ í•˜ë©´ì„œ ë°”ë¼ëŠ” ê²ƒì´ í•˜ë‚˜ëŠ” ìœ ì‚¬ì„±ì— ë”°ë¼ì„œ ê°€ê¹ê²Œ í•˜ê³ , ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ìœ ì‚¬ì„±ì— ë”°ë¼ì„œ ë©€ê²Œí•˜ì—¬ ë¶„ë¥˜í•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì´ë‹¤.

ì´ê²ƒì„ ì‹¤ì œë¡œ í™•ì¸í•˜ê¸° ìœ„í•´ì„œ ì—¬ëŸ¬ ë‹¤ì–‘í•œ ELECTRAë¥¼ êµ¬í˜„í•˜ë˜, í•´ë‹¹ ELECTRAë“¤ì€ token embeddingì„ ê³µìœ í•˜ì§€ ì•Šë„ë¡ êµ¬í˜„í–ˆë‹¤ê³ í•œë‹¤. ê·¸ë ‡ê²Œ êµ¬í˜„ì„ í•˜ë©´ No Embedding Sharing(NES)ê°€ ë˜ëŠ” ê²ƒì´ë‹¤. ì–˜ë„¤ëŠ” gradient updateê°€ ê°ê° ëœë‹¤. ìš°ì„ ì€ (1) generatorì˜ parameter(token embedding with $E_G$)ê°€ MLM lossë¥¼ back-propí•˜ë©´ì„œ ì—…ë°ì´íŠ¸ë˜ê³ , (2) ì´í›„ì— discriminatorê°€ generator outputì„ inputìœ¼ë¡œ ë°›ëŠ”ë‹¤ (3) ë§ˆì§€ë§‰ìœ¼ë¡œ discriminator parameter(token embeddings with $E_D$)ë¥¼ RTD lossë¥¼ back-propí•˜ë©´ì„œ updateí•œë‹¤.

ì´ë“¤ì€ 3ê°€ì§€ë¡œ ES vs NESë¥¼ ë¹„êµí–ˆë‹¤ê³  í•œë‹¤.

1. convergence speed : NESê°€ ë‹¹ì—°íˆ gradient conflictë¥¼ ë°©ì§€í•¨ìœ¼ë¡œ ìŠ¹ë¦¬
2. quality of token embeddings : average cosine similiarity scoresë¥¼ ë¹„êµí–ˆë‹¤. $E_G$ì—ì„œëŠ” êµ‰ì¥íˆ íš¨ê³¼ê°€ ì¢‹ì•˜ì§€ë§Œ $E_D$ëŠ” í•™ìŠµì„ ê±°ì˜ ëª»í•œ ê²ƒìœ¼ë¡œ ë³´ì˜€ë‹¤. íš¨ê³¼ê°€ ì¢‹ë‹¤ëŠ” ê²ƒì€ ì˜ë¯¸ì ìœ¼ë¡œ coherent ì¼ê´€ì„± ìˆê²Œ $E_G$ê°€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ê°€ ë§¤ìš° ë†’ì•„ì§€ëŠ” ê²ƒì„ ë§í•œë‹¤.
3. performance on downstream NLP tasks : ë˜í•œ NESê°€ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ testì—ì„œë„ ì¢‹ì€ ëª¨ìŠµì„ ë³´ì´ì§€ ëª»í–ˆë‹¤

> ESê°€ generator embeddingìœ¼ë¡œë¶€í„° discriminatorê°€ í•™ìŠµì„ í•  ë•Œ ë„ì›€ì„ ë°›ëŠ”ê²Œ ì¥ì ì´ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.

### ???? average cosine similiarity of word embeddings of the G vs D

average cosine similiarityê°€ ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ê²ƒì¸ê°€?
ì–´ë–¤ ì˜ë¯¸ì¸ì§€ ì œëŒ€ë¡œ ì´í•´ë¥¼ ëª»í•œ ê±° ê°™ë‹¤.

### 3.3 Gradient-Disentangled Embedding Sharing(GDES)

ES, NESì˜ ë‹¨ì ì„ ê½¤ ëš«ê¸° ìœ„í•´ í•´ë‹¹ ë…¼ë¬¸ì—ì„œ ì¤‘ìš”í•˜ê²Œ ë§í•˜ëŠ” ì§€ì ì´ë‹¤. ë‘ ê°œì˜ ì¥ ë‹¨ì ì´ ë¶„ëª…í•˜ê²Œ ì¡´ì¬í•˜ë©´ì„œ ë‘ ê°œë¥¼ ëª¨ë‘ ì±™ê¸¸ ë°©ë²•ìœ¼ë¡œ ë‚˜ì˜¨ ê²ƒì´ë‹¤. í•œ ë²ˆ ì •ë¦¬ë¥¼ í•˜ìë©´ ESëŠ” í•™ìŠµì€ ëŠë¦¬ì§€ë§Œ generator output : token embeddingë¥¼ discriminatorê°€ ì°¸ì¡°í•˜ë©´ì„œ í•™ìŠµ parameter reducingì— ë„ì›€ì„ ë°›ëŠ” ë‹¤ëŠ” ê²ƒì´ë‹¤. í•˜ì§€ë§Œ ë‹¨ì ì€ generator discriminator token embeddingsê°€ ë‘˜ ë‹¤ ì¼ê´€ì„±ì´ ì—†ì–´ì§„ë‹¤ëŠ” ê²ƒì´ë‹¤.

ë°˜ë©´ì— NESëŠ” í•™ìŠµì´ êµ‰ì¥íˆ ë¹¨ë¼ì§„ë‹¤. G,Dì˜ ë°©í–¥ì„±ì˜ ì •ë°˜ëŒ€ì˜ ì„±ì§ˆì„ í•´ê²°í•´ ì¤Œìœ¼ë¡œì¨ í•™ìŠµì´ ìš©ì´í•˜ê²Œ ëœë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ ê²°ë¡ ì ìœ¼ë¡œëŠ” í•™ìŠµì„ ì˜¤íˆë ¤ ESë³´ë‹¤ ëª»í•œ ê¼´ì´ ë‚œë‹¤. ê·¸ë˜ë„ ì¥ì ì€ Gì˜ token embeddingì´ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ê°€ ë†’ì€ ì¼ê´€ì„± ìˆëŠ” embeddingì„ ë§Œë“¤ì–´ì£¼ëŠ” ê²½í–¥ì„±ì„ ë§Œë“œëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤.

ì´ ëª¨ë“  ë‹¨ì ì„ ì»¤ë²„í•˜ê³  ë„ëŒ€ì²´ ì–´ë–»ê²Œ ì¥ì ë§Œ ë‚¨ê¸´ë‹¤ëŠ” ê²ƒì¸ê°€? ì¥ì ë§Œ ë‚¨ê¸´ë‹¤ë©´ í•™ìŠµì˜ ì†ë„ë„ ë¹¨ë¼ì§€ë©´ì„œ ë™ì‹œì— G,Dì˜ token embeddingì´ ìœ ì‚¬ì„±ì„ ìœ ì§€í•˜ëŠ” ê²ƒì´ ë  ê²ƒì´ë‹¤. í›„ìë¥¼ ë…¼ë¬¸ì—ì„œëŠ” 'learn from same vocabulary and leverage the rich semantic information encoded in the embeddings'ë¼ê³  ë§í•œë‹¤.

ì´ê²ƒì„ GDESëŠ” ì˜¤ì§ generator embeddingsë¥¼ MLL lossë§Œ ê°€ì§€ê³  ì—…ë°ì´íŠ¸í•¨ìœ¼ë¡œì¨, outputì˜ ì¼ê´€ì„±ê³¼ í†µì¼ì„±ì„ ìœ ì§€í•œë‹¤. ê·¸ê²ƒì„ ìˆ˜ì‹ì ìœ¼ë¡œ ë³´ë©´

$$
E_D = sg(E_G) + E_{\Delta}
$$

ì›ë˜ëŠ” NESì—ì„œëŠ” $E_D$ë¡œ ë°”ë¡œ backpropí•˜ë˜ ê²ƒì„ re-parameterizeí•˜ì—¬ discriminator embeddingë¥¼ ìƒˆë¡œ ì •ì˜í•œë‹¤. $sg$dì˜ ì—­í• ì€ $E_G$ì—ì„œ ë‚˜ì˜¨ gradientê°€ ê³„ì† í˜ëŸ¬ë“¤ì–´ê°€ëŠ” ê²ƒì„ ë§‰ê³ , residual embeddings $E_{\Delta}$ ë§Œì„ ì—…ë°ì´íŠ¸ í•˜ê²Œ í•˜ëŠ” ê²ƒì´ë‹¤. ì´ê²ƒì€ residual learningì—ì„œì˜ ì•„ì´ë””ì–´ì™€ êµ‰ì¥íˆ ìœ ì‚¬í•œ ê²ƒ ê°™ì€ë°!!!.

(1) G output -> input for discriminator = $E_G$\
(2) update $E_G$ $E_D$ with MLM loss\
(3) D run on G output\
(4) update $E_D$ with RTD loss with only $E_{\Delta}$\
(5) after training, $E_{\Delta}$ + $E_G$ = $E_D$

> ì…‹ì€ embedding sharingì˜ ì°¨ì´ë§Œ ìˆê³ , computation costì˜ ì°¨ì´ëŠ” ì—†ìœ¼ë‹¤.
> computation costì˜ ì°¨ì´ê°€ ì—†ì´ ì•„ì´ë””ì–´ë§Œìœ¼ë¡œ ì„±ëŠ¥ì˜ upì„ í•œ ê²ƒë„ resnetì´ë‘ ë¹„ìŠ·í•˜ë‹¤.

ì½”ì‚¬ì¸ í‰ê·  ìœ ì‚¬ë„ì—ì„œë„ ì°¨ì´ê°€ NESë³´ë‹¤ëŠ” ëœí•œë° ì´ëŠ” 'preserves more semantic information in the discriminator embeddings through the partial weight sharging'ì´ë¼ê³  ë§í•œë‹¤. ì—¬ê¸°ì„œ ë³´ì´ëŠ” partial weight sharingì´ $E_{\Delta}$ì´ë©° ì´ê²ƒì´ embeddingì˜ ì”ì°¨ë¥¼ í•™ìŠµí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì§„í–‰ë¨ìœ¼ë¡œì¨ í•™ìŠµì„ ìš©ì´í•˜ê²Œ ê°€ì ¸ê°”ë‹¤... ì •ë„ë¡œ ë³´ì¸ë‹¤.

## Conclusion

- pre-training paradigm for language models based on the combination of DeBERTa and ELECTRA, two state-of-the-art models that use relative position encoding and replaced token detection (RTD) respectively
- interference issue between the generator and the discriminator in the RTD framework which is well known as the â€œtug-of-warâ€ dynamics.
- GDES : the discriminator to leverage the semantic information encoded in the generatorâ€™s embedding layer without interfering with the generatorâ€™s gradients and thus improves the pre-training efï¬ciency
- a new way of sharing information between the generator and the discriminator in the RTD framework, which can be easily applied to other RTD-based language models
- debertav3-large : 1.37% on the GLUE average score
- ëª©ì  : `parameter-efï¬cient pre-trained language models`

## Reference

1. [kpmg notion - Deberta Review](https://kpmgkr.notion.site/DeBERTa-421579f197f840f48cc920f09cbbb70b)
2. [HF - lighthouse/mdeberta-v3-base-kor-further](https://huggingface.co/lighthouse/mdeberta-v3-base-kor-further)
3. [github - microsoft/DeBERTa](https://github.com/microsoft/DeBERTa)

```{raw} html
<script
   type="text/javascript"
   src="https://utteranc.es/client.js"
   async="async"
   repo="surdarla/surdarla.github.io"
   issue-term="pathname"
   theme="github-light"
   label="ğŸ’¬ comment"
   crossorigin="anonymous"
/>
```
